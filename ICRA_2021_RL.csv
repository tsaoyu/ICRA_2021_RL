Title,Abstract,Authors,Affiliations
Reset-Free Reinforcement Learning Via Multi-Task Learning: Learning Dexterous Manipulation Behaviors without Human Intervention,"Reinforcement Learning (RL) algorithms can in principle acquire complex robotic skills by learning from large amounts of data in the real world, collected via trial and error. However, most RL algorithms use a carefully engineered setup in order to collect data, requiring human supervision and intervention to provide episodic resets. To make data collection scalable, such applications require reset-free algorithms that are able to learn autonomously, without explicit instrumentation or human intervention. Most prior work in this area handles single-task learning. However, we might also want robots that can perform large repertoires of skills. The key observation we make is that an appropriately chosen multi-task RL setting actually alleviates the reset-free learning challenge, with minimal additional machinery required. In effect, solving a multi-task problem can directly solve the reset-free problem since different combinations of tasks can serve to reset other tasks. This type of multi-task learning can effectively scale reset-free learning schemes to much more complex problems, as we demonstrate in our experiments. We propose a scheme for multi-task learning that tackles the reset-free learning problem, and show its effectiveness at learning to solve complex dexterous manipulation tasks in both hardware and simulation without any explicit resets. This work shows the ability to learn in-hand manipulation behaviors in the real world with RL without any human intervention.
                           ","['Gupta, Abhishek', 'Yu, Justin', 'Zhao, Zihao', 'Kumar, Vikash', 'Aaron, Rovinsky', 'Xu, Kelvin', 'Devlin, Thomas', 'Levine, Sergey']","['UC Berkeley', 'UC Berkeley', 'UC Berkeley', 'Univ. of Washington', 'University of California, Berkeley', 'University of California, Berkeley', 'University of California, . Berkeley', 'UC Berkeley']"
DisCo RL: Distribution-Conditioned Reinforcement Learning for General-Purpose Policies,"Can we use reinforcement learning to instead learn general-purpose policies that can perform a wide range of different tasks, resulting in flexible and reusable skills? Contextual policies provide this capability in principle, but the representation of the context determines the degree of generalization and expressivity. Categorical contexts preclude generalization to entirely new tasks. Goal-conditioned policies may enable some generalization, but cannot capture all tasks that might be desired. In this paper, we propose goal distributions as a general and broadly applicable task representation suitable for contextual policies. Goal distributions are general in the sense that they can represent any state-based reward function when equipped with an appropriate distribution class, while the particular choice of distribution class allows us to trade off expressivity and learnability. We develop an off-policy algorithm called distribution-conditioned reinforcement learning (DisCo RL) to efficiently learn these policies. We evaluate DisCo RL on a variety of robot manipulation tasks and find that it significantly outperforms prior methods on tasks that require generalization to new goal distributions.
                           ","['Nasiriany, Soroush', 'Pong, Vitchyr', 'Nair, Ashvin', 'Khazatsky, Alexander', 'Berseth, Glen', 'Levine, Sergey']","['UC Berkeley', 'UC Berkeley', 'UC Berkeley', 'UC Berkeley', 'University of California Berkeley', 'UC Berkeley']"
What Can I Do Here? Learning New Skills by Imagining Visual Affordances,"A generalist robot equipped with learned skills must be able to perform many tasks in many different environments. However, zero-shot generalization to new settings is not always possible. When the robot encounters a new environment or object, it may need to finetune some of its previously learned skills to accommodate this change. But crucially, previously learned behaviors and models should still be suitable to accelerate this relearning. In this paper, we aim to study how generative models of possible outcomes can allow a robot to learn visual representations of affordances, so that the robot can sample potentially possible outcomes in new situations, and then further train its policy to achieve those outcomes. In effect, prior data is used to learn what kinds of outcomes may be possible, such that when the robot encounters an unfamiliar setting, it can sample potential outcomes from its model, attempt to reach them, and thereby update both its skills and its outcome model. We show that this approach can be used to train goal-conditioned policies that operate on raw image inputs, and can rapidly learn to manipulate new objects via our proposed affordance-directed exploration scheme.
                           ","['Khazatsky, Alexander', 'Nair, Ashvin', 'Jing, Daniel', 'Levine, Sergey']","['UC Berkeley', 'UC Berkeley', 'University of California, Berkeley', 'UC Berkeley']"
Reinforcement Learning for Robust Parameterized Locomotion Control of Bipedal Robots,"Developing robust walking controllers for bipedal robots is a challenging endeavor. Traditional model-based locomotion controllers require simplifying assumptions and careful modelling; any small errors can result in unstable control. To address these challenges for bipedal locomotion, we present a model-free reinforcement learning framework for training robust locomotion policies in simulation, which can then be transferred to a real bipedal Cassie robot. To facilitate sim-to-real transfer, domain randomization is used to encourage the policies to learn behaviors that are robust across variations in the system dynamics. The learned policies enable Cassie to perform a set of diverse and dynamic behaviors, while also being more robust than traditional controllers and prior learning-based methods that use residual control. We demonstrate this on versatile walking behaviors such as tracking a target walking velocity, walking height, and turning yaw.
                           ","['Li, Zhongyu', 'Cheng, Xuxin', 'Peng, Xue Bin', 'Abbeel, Pieter', 'Levine, Sergey', 'Berseth, Glen', 'Sreenath, Koushil']","['University of California, Berkeley', 'University of California, Berkeley', 'University of California, Berkeley', 'UC Berkeley', 'UC Berkeley', 'University of California Berkeley', 'University of California, Berkeley']"
Model-Based Meta-Reinforcement Learning for Flight with Suspended Payloads,"Transporting suspended payloads is challenging for autonomous aerial vehicles because the payload can cause significant and unpredictable changes to the robot's dynamics. These changes can lead to suboptimal flight performance or even catastrophic failure. Although adaptive control and learning-based methods can in principle adapt to changes in these hybrid robot-payload systems, rapid mid-flight adaptation to payloads that have a priori unknown physical properties remains an open problem. We propose a meta-learning approach that ``learns how to learn'' models of altered dynamics within seconds of post-connection flight data. Our experiments demonstrate that our online adaptation approach outperforms non-adaptive methods on a series of challenging suspended payload transportation tasks.
                           ","['Belkhale, Suneel', 'Kahn, Gregory', 'McAllister, Rowan', 'Calandra, Roberto', 'Levine, Sergey']","['Stanford University', 'University of California, Berkeley', 'University of California, Berkeley', 'Facebook', 'UC Berkeley']"
ViNG: Learning Open-World Navigation with Visual Goals,"We propose a learning-based navigation system for reaching visually indicated goals and demonstrate this system on a real mobile robot platform. Learning provides an appealing alternative to conventional methods for robotic navigation: instead of reasoning about environments in terms of geometry and maps, learning can enable a robot to learn about navigational affordances, understand what types of obstacles are traversable (e.g., tall grass) or not (e.g., walls), and generalize over patterns in the environment. However, unlike conventional planning algorithms, it is harder to change the goal for a learned policy during deployment. We propose a method for learning to navigate towards a goal image of the desired destination. By combining a learned policy with a topological graph constructed out of previously observed data, our system can determine how to reach this visually indicated goal even in the presence of variable appearance and lighting. Three key insights, waypoint proposal, graph pruning and negative mining, enable our method to learn to navigate in real-world environments using only offline data, a setting where prior methods struggle. We instantiate our method on a real outdoor ground robot and show that our system (ViNG) outperforms previously-proposed methods for goal-conditioned reinforcement learning. We also study how ViNG generalizes to unseen environments and evaluate its ability to adapt to such an environment with growing experience.
                           ","['Shah, Dhruv', 'Eysenbach, Benjamin', 'Kahn, Gregory', 'Rhinehart, Nicholas', 'Levine, Sergey']","['University of California, Berkeley', 'CMU', 'University of California, Berkeley', 'Carnegie Mellon University', 'UC Berkeley']"
BADGR: An Autonomous Self-Supervised Learning-Based Navigation System,"Mobile robot navigation is typically regarded as a geometric problem, in which the robot's objective is to perceive the geometry of the environment in order to plan collision-free paths towards a desired goal. However, a purely geometric view of the world can be insufficient for many navigation problems. For example, a robot navigating based on geometry may avoid a field of tall grass because it believes it is untraversable, and will therefore fail to reach its desired goal. In this work, we investigate how to move beyond these purely geometric-based approaches using a method that learns about physical navigational affordances from experience. Our reinforcement learning approach, which we call BADGR, is an end-to-end learning-based mobile robot navigation system that can be trained with autonomously-labeled off-policy data gathered in real-world environments, without any simulation or human supervision. BADGR can navigate in real-world urban and off-road environments with geometrically distracting obstacles. It can also incorporate terrain preferences, generalize to novel environments, and continue to improve autonomously by gathering more data. Videos, code, and other supplemental material are available on our website https://sites.google.com/view/badgr
                           ","['Kahn, Gregory', 'Abbeel, Pieter', 'Levine, Sergey']","['University of California, Berkeley', 'UC Berkeley', 'UC Berkeley']"
SimGAN: Hybrid Simulator Identification for Domain Adaptation Via Adversarial Reinforcement Learning,"As learning-based approaches progress towards automating robot controllers design, transferring learned policies to new domains with different dynamics (e.g. sim-to-real transfer) still demands manual effort. This paper introduces SimGAN, a framework to tackle domain adaptation by identifying a hybrid physics simulator to match the simulated trajectories to the ones from the target domain, using a learned discriminative loss to address the limitations associated with manual loss design. Our hybrid simulator combines neural networks and traditional physics simulation to balance expressiveness and generalizability, and alleviates the need for a carefully selected parameter set in System ID. Once the hybrid simulator is identified via adversarial reinforcement learning, it can be used to refine policies for the target domain, without the need to interleave data collection and policy refinement. We show that our approach outperforms multiple strong baselines on six robotic locomotion tasks for domain adaptation.
                           ","['Jiang, Yifeng', 'Zhang, Tingnan', 'Ho, Daniel', 'Bai, Yunfei', 'Liu, Karen', 'Levine, Sergey', 'Tan, Jie']","['Stanford University', 'Google', 'Google X', 'Google X', 'Stanford University', 'UC Berkeley', 'Google']"
Reactive Navigation in Crowds for Non-Holonomic Robots with Convex Bounding Shape,"This paper describes a novel method for non-holonomic robots of convex shape to avoid imminent collisions with moving obstacles. The method's purpose is to assist navigation in crowds by correcting steering from the robot's path planner or driver. We evaluate its performance using a custom simulator which replicates real crowd movements and corresponding metrics which quantify the agent's efficiency and the robot's impact on the crowd and count collisions. We implement and evaluate the method on the standing wheelchair Qolo. In our experiments, it drives in autonomous mode using on-board sensing (LiDAR, RGB-D camera and a system to track pedestrians) and avoids collisions with up to five pedestrians and passes through a door.
                           ","['Kahn, Gregory', 'Abbeel, Pieter', 'Levine, Sergey']","['University of California, Berkeley', 'UC Berkeley', 'UC Berkeley']"
Learning Task Space Actions for Bipedal Locomotion,"Recent work has demonstrated the success of reinforcement learning (RL) for training bipedal locomotion policies for real robots. This prior work, however, has focused on learning joint-coordination controllers based on an objective of following joint trajectories produced by already available controllers. As such, it is difficult to train these approaches to achieve higher-level goals of legged locomotion, such as simply specifying the desired end-effector foot movement or ground reaction forces. In this work, we propose an approach for integrating knowledge of the robot system into RL to allow for learning at the level of task space actions in terms of feet setpoints. In particular, we integrate learning a task space policy with a model-based inverse dynamics controller, which translates task space actions into joint-level controls. With this natural action space for learning locomotion, the approach is more sample efficient and produces desired task space dynamics compared to learning purely joint space actions. We demonstrate the approach in simulation and also show that the learned policies are able to transfer to the real bipedal robot Cassie. This result encourages further research towards incorporating bipedal control techniques into the structure of the learning process to enable dynamic behaviors.
                           ","['Duan, Helei', 'Dao, Jeremy', 'Green, Kevin', 'Apgar, Taylor', 'Fern, Alan', 'Hurst, Jonathan']","['Oregon State University', 'Oregon State University', 'Oregon State University', 'Agility Robotics', 'Oregon State University', 'Oregon State University']"
Grasp Analysis and Manipulation Kinematics for Isoperimetric Truss Robots,"Soft truss robots have demonstrated an ability to grasp and manipulate objects using the members of their structure. The compliance of the members affords large contact areas with even force distribution, allowing for successful grasping even with imprecise open-loop control. In this work we present methods of analyzing and controlling isoperimetric truss robots in the context of grasping and manipulating objects. We use a direct stiffness model to characterize the structural properties of the robot and its interactions with external objects. With this approach we can estimate grasp forces and stiffnesses with limited computation compared to higher fidelity finite elements methods, which, given the high degree-of-freedom of truss robots, are prohibitively expensive to run on-board. In conjunction with the structural model, we build upon a literature of differential kinematics for truss robots and apply it to the task of manipulating an object within the robot's workspace.
                           ","['Du, Yuqing', 'Watkins, Olivia', 'Pathak, Deepak', 'Abbeel, Pieter', 'Darrell, Trevor']","['UC Berkeley', 'UC Berkeley', 'Carnegie Mellon University', 'UC Berkeley', 'UC Berkeley']"
Uncertainty-Aware Non-Linear Model Predictive Control for Human-Following Companion Robot,"For a companion robot that follows a person as an assistant, predicting human walking is important to produce a proactive movement that is helpful to maintain an appropriate area decided by the human personal space. However, fully trusting the prediction may result in obstructing human walking because it is not always accurate. Hence, we consider the estimation of uncertainty (i.e., entropy) of the prediction to enable the robot to move without causing overconfident motion and without being late for the person it follows. To consider this uncertainty of the prediction to the controller, we introduce a reliability value that changes based on the entropy of the prediction. This value expresses the extent the controller should trust the prediction result, and it affects the cost function of our controller. We propose an uncertainty-aware robot controller based on nonlinear model predictive control to realize natural human-followings. We found that our uncertainty-aware control system can produce an appropriate robot movement, such as not obstructing the human walking and avoiding delay, in both simulations using actual human walking data and real-robot experiments.
                           ","['Siekmann, Jonah', 'Godse, Yesh', 'Fern, Alan', 'Hurst, Jonathan']","['Oregon State University', 'Oregon State University', 'Oregon State University', 'Oregon State University']"
Learning Spring Mass Locomotion: Guiding Policies with a Reduced-Order Model,"In this paper, we describe an approach to achieve dynamic legged locomotion on physical robots which combines existing methods for control with reinforcement learning. Specifically, our goal is a control hierarchy in which highest-level behaviors are planned through reduced-order models, which describe the fundamental physics of legged locomotion, and lower level controllers utilize a learned policy that can bridge the gap between the idealized, simple model and the complex, full order robot. The high-level planner can use a model of the environment and be task specific, while the low-level learned controller can execute a wide range of motions so that it applies to many different tasks. In this letter we describe this learned dynamic walking controller and show that a range of walking motions from reduced-order models can be used as the command and primary training signal for learned policies. The resulting policies do not attempt to naively track the motion (as a traditional trajectory tracking controller would) but instead balance immediate motion tracking with long term stability. The resulting controller is demonstrated on a human scale, unconstrained, untethered bipedal robot at speeds up to 1.2 m/s. This letter builds the foundation of a generic, dynamic learned walking controller that can be applied to many different tasks.
                           ","['Green, Kevin', 'Godse, Yesh', 'Dao, Jeremy', 'Hatton, Ross', 'Fern, Alan', 'Hurst, Jonathan']","['Oregon State University', 'Oregon State University', 'Oregon State University', 'Oregon State University', 'Oregon State University', 'Oregon State University']"
Auto-Tuned Sim-To-Real Transfer,"Policies trained in simulation often fail when transferred to the real world due to the ¡®reality gap¡¯ where the simulator is unable to accurately capture the dynamics and visual properties of the real world. Current approaches to tackle this problem, such as domain randomization, require prior knowledge and engineering to determine how much to randomize system parameters in order to learn a policy that is robust to sim-to-real transfer while also not being too conservative. We propose a method for automatically tuning simulator system parameters to match the real world using only raw RGB images of the real world without the need to define rewards or estimate state. Our key insight is to reframe the auto-tuning of parameters as a search problem where we iteratively shift the simulation system parameters to approach the real world system parameters. We propose a Search Param Model (SPM) that, given a sequence of observations and actions and a set of system parameters, predicts whether the given parameters are higher or lower than the true parameters used to generate the observations. We evaluate our method on multiple robotic control tasks in both sim-to-sim and sim-toreal transfer, demonstrating significant improvement over naive domain randomization. Project videos at https://yuqingd. github.io/autotuned-sim2real/.
                           ","['Du, Yuqing', 'Watkins, Olivia', 'Darrell, Trevor', 'Abbeel, Pieter', 'Pathak, Deepak']","['UC Berkeley', 'UC Berkeley', 'UC Berkeley', 'UC Berkeley', 'Carnegie Mellon University']"
Grasp Analysis and Manipulation Kinematics for Isoperimetric Truss Robots,"Soft truss robots have demonstrated an ability to grasp and manipulate objects using the members of their structure. The compliance of the members affords large contact areas with even force distribution, allowing for successful grasping even with imprecise open-loop control. In this work we present methods of analyzing and controlling isoperimetric truss robots in the context of grasping and manipulating objects. We use a direct stiffness model to characterize the structural properties of the robot and its interactions with external objects. With this approach we can estimate grasp forces and stiffnesses with limited computation compared to higher fidelity finite elements methods, which, given the high degree-of-freedom of truss robots, are prohibitively expensive to run on-board. In conjunction with the structural model, we build upon a literature of differential kinematics for truss robots and apply it to the task of manipulating an object within the robot's workspace.
                           ","['Duan, Helei', 'Dao, Jeremy', 'Green, Kevin', 'Apgar, Taylor', 'Fern, Alan', 'Hurst, Jonathan']","['Oregon State University', 'Oregon State University', 'Oregon State University', 'Agility Robotics', 'Oregon State University', 'Oregon State University']"
Sim-To-Real Learning of All Common Bipedal Gaits Via Periodic Reward Composition,"We study the problem of realizing the full spectrum of bipedal locomotion on a real robot with sim-to-real reinforcement learning (RL). A key challenge of learning legged locomotion is describing different gaits, via reward functions, in a way that is intuitive for the designer and specific enough to reliably learn the gait across different initial random seeds or hyperparameters. A common approach is to use reference motions (e.g. trajectories of joint positions) to guide learning. However, finding high-quality reference motions can be difficult and the trajectories themselves narrowly constrain the space of learned motion. At the other extreme, reference-free reward functions are often underspecified (e.g. move forward) leading to massive variance in policy behavior, or are the product of significant reward-shaping via trial-and-error, making them exclusive to specific gaits. In this work, we propose a reward-specification framework based on composing simple probabilistic periodic costs on basic forces and velocities. We instantiate this framework to define a parametric reward function with intuitive settings for all common bipedal gaits - standing, walking, hopping, running, and skipping. Using this function we demonstrate successful sim-to-real transfer of the learned gaits to the bipedal robot Cassie, as well as a generic policy that can transition between all of the two-beat gaits.
                           ","['Siekmann, Jonah', 'Godse, Yesh', 'Fern, Alan', 'Hurst, Jonathan']","['Oregon State University', 'Oregon State University', 'Oregon State University', 'Oregon State University']"
Contextual Latent-Movements Off-Policy Optimization for Robotic Manipulation Skills,"Parameterized movement primitives have been extensively used for imitation learning of robotic tasks. However, the high-dimensionality of the parameter space hinders the improvement of such primitives in the reinforcement learning (RL) setting, especially for learning with physical robots. In this paper we propose a novel view on handling the demonstrated trajectories for acquiring low-dimensional, non-linear latent dynamics, using mixtures of probabilistic principal component analyzers (MPPCA) on the movements' parameter space. Moreover, we introduce a new contextual off-policy RL algorithm, named LAtent-Movements Policy Optimization (LAMPO). LAMPO can provide gradient estimates from previous experience using self-normalized importance sampling, hence, making full use of samples collected in previous learning iterations. These advantages combined provide a complete framework for sample-efficient off-policy optimization of movement primitives for robot learning of high-dimensional manipulation skills. Our experimental results conducted both in simulation and on a real robot show that LAMPO provides sample-efficient policies against common approaches in literature.
                           ","['Tosatto, Samuele', 'Chalvatzaki, Georgia', 'Peters, Jan']","['Technical University Darmstadt', 'Technische Universit?t Darmastadt, Intelligent Autonomous Syste', 'Technische Universit?t Darmstadt']"
Data-Efficient Domain Randomization with Bayesian Optimization,"When learning policies for robot control, the required real-world data is typically prohibitively expensive to acquire, so learning in simulation is a popular strategy. Unfortunately, such polices are often not transferable to the real world due to a mismatch between the simulation and reality, called ¡®reality gap¡¯. Domain randomization methods tackle this problem by randomizing the physics simulator (source domain) during training according to a distribution over domain parameters in order to obtain more robust policies that are able to overcome the reality gap. Most domain randomization approaches sample the domain parameters from a fixed distribution. This solution is suboptimal in the context of sim-to-real transferability, since it yields policies that have been trained without explicitly optimizing for the reward on the real system (target domain). Additionally, a fixed distribution assumes there is prior knowledge about the uncertainty over the domain parameters. In this paper, we propose Bayesian Domain Randomization (BayRn), a black-box sim-to-real algorithm that solves tasks efficiently by adapting the domain parameter distribution during learning given sparse data from the real-world target domain. BayRn uses Bayesian optimization to search the space of source domain distribution parameters such that this leads to a policy which maximizes the real-word objective, allowing for adaptive distributions during policy optimization.
                           ","['Muratore, Fabio', 'Eilers, Christian', 'Gienger, Michael', 'Peters, Jan']","['TU Darmstadt', 'Technische Universit?t Darmstadt', 'Honda Research Institute Europe', 'Technische Universit?t Darmstadt']"
Model Predictive Actor-Critic: Accelerating Robot Skill Acquisition with Deep Reinforcement Learning,"Substantial advancements to model-based reinforcement learning algorithms have been impeded by the model-bias induced by the collected data, which generally hurts performance. Meanwhile, their inherent sample efficiency warrants utility for most robot applications, limiting potential damage to the robot and its environment during training. Inspired by information theoretic model predictive control and advances in deep reinforcement learning, we introduce Model Predictive Actor-Critic (MoPAC), a hybrid model-based/model-free method that combines model predictive rollouts with policy optimization as to mitigate model bias. MoPAC leverages optimal trajectories to guide policy learning, but explores via its model-free method, allowing the algorithm to learn more expressive dynamics models. This combination guarantees optimal skill learning up to an approximation error and reduces necessary physical interaction with the environment, making it suitable for real-robot training. We provide extensive results showcasing how our proposed method generally outperforms current state-of-the-art and conclude by evaluating MoPAC for learning on a physical robotic hand performing valve rotation and finger gaiting--a task that requires grasping, manipulation, and then regrasping of an object.
                           ","['Morgan, Andrew', 'Nandha, Daljeet', 'Chalvatzaki, Georgia', ""D'Eramo, Carlo"", 'Dollar, Aaron', 'Peters, Jan']","['Yale University', 'Technische Universit?t Darmstadt', 'Technische Universit?t Darmastadt, Intelligent Autonomous Syste', 'TU Darmstadt', 'Yale University', 'Technische Universit?t Darmstadt']"
Differentiable Physics Models for Real-World Offline Model-Based Reinforcement Learning,"A limitation of model-based reinforcement learning (MBRL) is the exploitation of errors in the learned models. Black-box models can fit complex dynamics with high fidelity, but their behavior is undefined outside of the data distribution. Physics-based models are better at extrapolating, due to the general validity of their informed structure, but underfit in the real world due to the presence of unmodeled phenomena. In this work, we demonstrate experimentally that for the emph{offline} model-based reinforcement learning setting, physics-based models can be beneficial compared to high-capacity function approximators if the mechanical structure is known. Physics-based models can learn to perform the ball in a cup (BiC) task on a physical manipulator using only 4 minutes of sampled data using offline MBRL. We find that black-box models consistently produce unviable policies for BiC as all predicted trajectories diverge to physically impossible state, despite having access to more data than the physics-based model. In addition, we generalize the approach of physics parameter identification from modeling holonomic multi-body systems to systems with nonholonomic dynamics using end-to-end automatic differentiation.Videos: https://sites.google.com/view/ball-in-a-cup-in-4-minutes/
                           ","['Lutter, Michael', 'Silberbauer, Johannes', 'Watson, Joe', 'Peters, Jan']","['Technische Universitaet Darmstadt', 'Technical University of Darmstadt', 'Technical University Darmstadt', 'Technische Universit?t Darmstadt']"
LEAF: Latent Exploration Along the Frontier,"Self-supervised goal proposal and reaching is a key component for exploration and efficient policy learning algorithms. Such a self-supervised approach without access to any oracle goal sampling distribution requires deep exploration and commitment so that long horizon plans can be efficiently discovered. In this paper, we propose an exploration framework, which learns a dynamics-aware manifold of reachable states. For a goal, our proposed method deterministically visits a state at the current frontier of reachable states (commitment/reaching) and then stochastically explores to reach the goal (exploration). This allocates exploration budget near the frontier of the reachable region instead of its interior. We target the challenging problem of policy learning from initial and goal states specified as images, and do not assume any access to the underlying ground-truth states of the robot and the environment. To keep track of reachable latent states, we propose a distance-conditioned reachability network that is trained to infer whether one state is reachable from another within the specified latent space distance. Given an initial state, we obtain a frontier of reachable states from that state. By incorporating a curriculum for sampling easier goals (closer to the start state) before more difficult goals, we demonstrate that the proposed self-supervised exploration algorithm, superior performance compared to existing baselines on a set of challenging robotic environments.
                           ","['Bharadhwaj, Homanga', 'Garg, Animesh', 'Shkurti, Florian']","['University of Toronto, Canada', 'University of Toronto', 'University of Toronto']"
Shaping Rewards for Reinforcement Learning with Imperfect Demonstrations Using Generative Models,"The potential benefits of model-free reinforcement learning to real robotics systems are limited by its uninformed exploration that leads to slow convergence, lack of data-efficiency, and unnecessary interactions with the environment. To address these drawbacks we propose a method that combines reinforcement and imitation learning by shaping the reward function with a state-and-action-dependent potential that is trained from demonstration data, using a generative model. We show that this accelerates policy learning by specifying high-value areas of the state and action space that are worth exploring first. Unlike the majority of existing methods that assume optimal demonstrations and incorporate the demonstration data as hard constraints on policy optimization, we instead incorporate demonstration data as advice in the form of a reward shaping potential trained as a generative model of states and actions. In particular, we examine both normalizing flows and Generative Adversarial Networks to represent these potentials. We show that, unlike many existing approaches that incorporate demonstrations as hard constraints, our approach is unbiased even in the case of suboptimal and noisy demonstrations. We present an extensive range of simulations, as well as experiments on the Franka Emika 7DOF arm, to demonstrate the practicality of our method.
                           ","['Wu, Yuchen', 'Mozifian, Melissa', 'Shkurti, Florian']","['University of Toronto', 'McGill University', 'University of Toronto']"
Instance-Aware Predictive Navigation in Multi-Agent Environments,"In this work, we aim to achieve ef?cient end-to-end learning of driving policies in dynamic multi-agent environments. Predicting and anticipating future events at the object level are critical for making informed driving decisions. We propose an Instance-Aware Predictive Control (IPC) approach, which forecasts interactions between agents as well as future scene structures. We adopt a novel multi-instance event prediction module to estimate the possible interaction among agents in the ego-centric view, conditioned on the selected action sequence of the ego-vehicle. To decide the action at each step, we seek the action sequence that can lead to safe future states based on the prediction module outputs by repeatedly sampling likely action sequences. We design a sequential action sampling strategy to better leverage predicted states on both scene-level and instance-level. Our method establishes a new state of the art in the challenging CARLA multi-agent driving simulation environments without expert demonstration, giving better explainability and sample efficiency.
                           ","['Cao, Jinkun', 'Wang, Xin', 'Darrell, Trevor', 'Yu, Fisher']","['Carnegie Mellon University', 'UC Berkeley', 'UC Berkeley', 'ETH Zurich']"
Deep Affordance Foresight: Planning through What Can Be Done in the Future,"Planning in realistic environments requires searching in large planning spaces. Affordances are a powerful concept to simplify this search, because they model what actions can be successful in a given situation. However, the classical notion of affordance is not suitable for long horizon planning because it only informs the robot about the immediate outcome of actions instead of what actions are best for achieving a long-term goal. In this paper, we introduce a new affordance representation that enables the robot to reason about the long-term effects of actions through modeling what actions are afforded in the future, thereby informing the robot the best actions to take next to achieve a task goal. Based on the new representation, we develop a learning-to-plan method, Deep Affordance Foresight (DAF), that learns partial environment models of affordances of parameterized motor skills through trial-and-error. We evaluate DAF on two challenging manipulation domains and show that it can effectively learn to carry out multi-step tasks, share learned affordance representations among different tasks, and learn to plan with high-dimensional image inputs.
                           ","['Xu, Danfei', 'Mandlekar, Ajay Uday', 'Mart¨ªn-Mart¨ªn, Roberto', 'Zhu, Yuke', 'Savarese, Silvio', 'Fei-Fei, Li']","['Stanford Univesity', 'Stanford University', 'Stanford University', 'The University of Texas at Austin', 'Stanford University', 'Stanford University']"
Continual Model-Based Reinforcement Learning with Hypernetworks,"Effective planning in model-based reinforcement learning (MBRL) and model-predictive control (MPC) relies on the accuracy of the learned dynamics model. In many instances of MBRL and MPC, this model is assumed to be stationary and is periodically re-trained from scratch on state transition experience collected from the beginning of environment interactions. This implies that the time required to train the dynamics model ¨C and the pause required between plan executions ¨C grows linearly with the size of the collected experience. We argue that this is too slow for lifelong robot learning and propose HyperCRL, a method that continually learns the encountered dynamics in a sequence of tasks using task-conditional hypernetworks. Our method has three main attributes: first, it includes dynamics learning sessions that do not revisit training data from previous tasks, so it only needs to store the most recent fixed-size portion of the state transition experience; second, it uses fixed-capacity hypernetworks to represent non-stationary and task-aware dynamics; third, it outperforms existing continual learning alternatives that rely on fixed-capacity networks, and does competitively with baselines that remember an ever increasing coreset of past experience. We show that HyperCRL is effective in continual model-based reinforcement learning in robot locomotion and manipulation scenarios, such as tasks involving pushing and door opening.
                           ","['Huang, Yizhou', 'Xie, Kevin', 'Bharadhwaj, Homanga', 'Shkurti, Florian']","['University of Toronto', 'University of Toronto', 'University of Toronto, Canada', 'University of Toronto']"
ReLMoGen: Integrating Motion Generation in Reinforcement Learning for Mobile Manipulation,"Many Reinforcement Learning (RL) approaches use joint control signals (positions, velocities, torques) as action space for continuous control tasks. We propose to lift the action space to a higher level in the form of subgoals for a motion generator. We argue that, by lifting the action space and by leveraging sampling-based motion planners, we can efficiently use RL to solve complex, long-horizon tasks that could not be solved with existing RL methods in the original action space. We propose ReLMoGen -- a framework that combines a learned policy to predict subgoals and a motion generator to plan and execute the motion needed to reach these subgoals. To validate our method, we apply ReLMoGen to two types of tasks: 1) Interactive Navigation tasks, navigation problems where interactions with the environment are required to reach the destination, and 2) Mobile Manipulation tasks, manipulation tasks that require moving the robot base. These problems are challenging because they are usually long-horizon, hard to explore during training, and comprise alternating phases of navigation and interaction. Our method is benchmarked on a diverse set of seven robotics tasks in photo-realistic simulation environments. In all settings, ReLMoGen outperforms state-of-the-art RL and Hierarchical RL baselines. ReLMoGen also shows outstanding transferability between different motion generators at test time, indicating a great potential to transfer to real robots.
                           ","['Xia, Fei', 'Li, Chengshu', 'Mart¨ªn-Mart¨ªn, Roberto', 'Litany, Or', 'Toshev, Alexander', 'Savarese, Silvio']","['Stanford University', 'Stanford University', 'Stanford University', 'Nvidia', 'Google', 'Stanford University']"
Policy Transfer Via Kinematic Domain Randomization and Adaptation,"Transferring reinforcement learning policies trained in physics simulation to the real hardware remains a challenge, known as the ``sim-to-real'' gap. Domain randomization is a simple yet effective technique to address dynamics discrepancies across source and target domains, but its success generally depends on heuristics and trial-and-error. In this work we investigate the impact of randomized parameter selection on policy transferability across different types of domain discrepancies. Contrary to common practice in which kinematic parameters are carefully measured while dynamic parameters are randomized, we found that virtually randomizing kinematic parameters (e.g., link lengths) during training in simulation generally outperforms dynamic randomization. Based on this finding, we introduce a new domain adaptation algorithm that utilizes simulated kinematic parameters variation. Our algorithm, Multi-Policy Bayesian Optimization, trains an ensemble of universal policies conditioned on virtual kinematic parameters and efficiently adapts to the target environment using a limited number of target domain rollouts. We showcase our findings on a simulated quadruped robot in five different target environments covering different aspects of domain discrepancies.
                           ","['Exarchos, Ioannis', 'Jiang, Yifeng', 'Yu, Wenhao', 'Liu, Karen']","['Stanford University', 'Stanford University', 'Georgia Institute of Technology', 'Stanford University']"
Robot Navigation in Constrained Pedestrian Environments Using Reinforcement Learning,"Navigating fluently around pedestrians is a necessary capability for mobile robots deployed in human environments, such as buildings and homes. While research on social navigation has focused mainly on the scalability with the number of pedestrians in open spaces, typical indoor environments present the additional challenge of constrained spaces such as corridors and doorways that limit maneuverability and influence patterns of pedestrian interaction. We present an approach based on reinforcement learning (RL) to learn policies capable of dynamic adaptation to the presence of moving pedestrians while navigating between desired locations in constrained environments. The policy network receives guidance from a motion planner that provides waypoints to follow a globally planned trajectory, whereas RL handles the local interactions. We explore a compositional principle for multi-layout training and find that policies trained in a small set of geometrically simple layouts successfully generalize to more complex unseen layouts that exhibit composition of the structural elements available during training. Going beyond walls-world like domains, we show transfer of the learned policy to unseen 3D reconstructions of two real environments (market, home). These results support the applicability of the compositional principle to navigation in real-world buildings and indicate promising usage of multi-agent simulation within reconstructed environments for tasks that involve interaction.
                           ","[""P¨¦rez-D'Arpino, Claudia"", 'Liu, Can', 'Goebel, Patrick', 'Mart¨ªn-Mart¨ªn, Roberto', 'Savarese, Silvio']","['Stanford University', 'Stanford University', 'Stanford University', 'Stanford University', 'Stanford University']"
Towards Multi-Modal Perception-Based Navigation: A Deep Reinforcement Learning Method,"In this paper, we present a novel navigation system of unmanned ground vehicle (UGV) for local path planning based on deep reinforcement learning. The navigation system decouples perception from control and takes advantage of multi-modal perception for a reliable online interaction with the surrounding environment of the UGV, which enables a direct policy learning for generating flexible actions to avoid collisions with obstacles in the navigation. By replacing the raw RGB images with their semantic segmentation maps as the input and applying a multi-modal fusion scheme, our system trained only in simulation can handle real-world scenes containing dynamic obstacles such as vehicles and pedestrians. We also introduce a modal separation learning to accelerate the training and further boost the performance. Extensive experiments demonstrate that our method closes the gap between simulated and real environments, exhibiting the superiority over state-of-the-art approaches.
                           ","['Huang, Xueqin', 'Deng, Han', 'Zhang, Wei', 'Song, Ran', 'Li, Yibin']","['Shandong University', 'Shandong University', 'Shandong University', 'Shandong University', 'Shandong University']"
Protective Policy Transfer,"Being able to transfer existing skills to new situations is a key capability when training robots to operate in unpredictable real-world environments. A successful transfer algorithm should not only minimize the number of samples that the robot needs to collect in the new environment, but also prevent the robot from damaging itself or the surrounding environment during the transfer process. In this work, we introduce a policy transfer algorithm for adapting robot locomotion skills to novel scenarios while minimizing serious failures. Our algorithm trains two control policies in the training environment: a task policy that is optimized to complete the task of interest, and a protective policy that is dedicated to keep the robot from unsafe events (e.g. falling to the ground). To decide which policy to use during execution, we learn a safety estimator model in the training environment that estimates a continuous safety level of the robot. When used with a set of thresholds, the safety estimator becomes a classifier for switching between the protective policy and the task policy. We evaluate our approach on four simulated robot locomotion problems and show that our method can achieve successful transfer to notably different environments while taking the robot's safety into consideration.
                           ","['Yu, Wenhao', 'Liu, Karen', 'Turk, Greg']","['Georgia Institute of Technology', 'Stanford University', 'Georgia Institute of Technology']"
Autonomous Multi-View Navigation Via Deep Reinforcement Learning,"In this paper, we propose a novel deep reinforcement learning (DRL) system for the autonomous navigation of mobile robots that consists of three modules: map navigation, multi-view perception and multi-branch control. Our DRL system takes as the input a routed map provided by a global planner and three RGB images captured by a multi-camera setup to gather global and local information, respectively. In particular, we present a multi-view perception module based on an attention mechanism to filter out redundant information caused by multi-camera sensing. We also replace raw RGB images with low-dimensional representations via a specifically designed network, which benefits a more robust sim2real transfer learning. Extensive experiments in both simulated and real-world scenarios demonstrate that our system outperforms state-of-the-art approaches.
                           ","['Huang, Xueqin', 'Chen, Wei', 'Zhang, Wei', 'Song, Ran', 'Cheng, Jiyu', 'Li, Yibin']","['Shandong University', 'Shandong University', 'Shandong University', 'Shandong University', 'Shandong University', 'Shandong University']"
Learning Multi-Object Dense Descriptor for Autonomous Goal-Conditioned Grasping,"In a goal-conditioned grasping task, a robot is asked to grasp the objects designated by a user. Existing methods for goal-conditioned grasping either can only handle relatively simple scenes or require extra user annotations. This paper proposes an autonomous method to enable the grasping of target object in a challenging yet general scene that contains multiple objects of different classes. It can effectively learn a dense descriptor and integrate it with a newly designed grasp affordance model. The proposed method is a self-supervised pipeline trained without any human supervision or robotic sampling. We validate our method via both simulated and real-world experiments while the training relies only on a variety of synthetic data, demonstrating a good generalization capability.
                           ","['Yang, Shuo', 'Zhang, Wei', 'Song, Ran', 'Cheng, Jiyu', 'Li, Yibin']","['Shandong University', 'Shandong University', 'Shandong University', 'Shandong University', 'Shandong University']"
Learning Agile Locomotion Skills with a Mentor,"Developing agile behaviors for legged robots remains a challenging problem. While deep reinforcement learning is a promising approach, learning truly agile behaviors typically requires tedious reward shaping and careful curriculum design. We formulate agile locomotion as a multi-stage learning problem in which a mentor guides the agent throughout the training. The mentor is optimized to place a checkpoint to guide the movement of the robot's center of mass while the student (i.e. the robot) learns to reach these checkpoints. Once the student can solve the task, we teach the student to perform the task without the mentor. We evaluate our proposed learning system with a simulated quadruped robot on a course consisting of randomly generated gaps and hurdles. Our method significantly outperforms a single-stage RL baseline without a mentor, and the quadruped robot can agilely run and jump across gaps and obstacles. Finally, we present a detailed analysis of the learned behaviors' feasibility and efficiency.
                           ","['Iscen, Atil', 'Yu, George', 'Escontrela, Alejandro', 'Jain, Deepali', 'Tan, Jie', 'Caluwaerts, Ken']","['Google', 'Google', 'Google', 'Robotics at Google', 'Google', 'Google']"
Multimodal Safety-Critical Scenarios Generation for Decision-Making Algorithms Evaluation,"Abstract¡ªExisting neural network-based autonomous systems are shown to be vulnerable against adversarial attacks, therefore sophisticated evaluation of their robustness is of great importance. However, evaluating the robustness under the worst-case scenarios based on known attacks is not comprehensive, not to mention that some of them even rarely occur in the real world. Also, the distribution of safety-critical data is usually multimodal, while most traditional attacks and evaluation methods focus on a single modality. To solve the above challenges, we propose a flow-based multimodal safety-critical scenario generator for evaluating decision-making algorithms. The proposed generative model is optimized with weighted likelihood maximization and a gradient-based sampling procedure is integrated to improve the sampling efficiency. The safety-critical scenarios are generated by efficiently querying the task algorithms and a simulator. Experiments on a self-driving task demonstrate our advantages in terms of testing efficiency and multimodal modeling capability. We evaluate six Reinforcement Learning algorithms with our generated traffic scenarios and provide empirical conclusions about their robustness.
                           ","['Ding, Wenhao', 'Chen, Baiming', 'Li, Bo', 'Kim, Ji Eun', 'Zhao, Ding']","['Carnegie Mellon University', 'Tsinghua University', 'University of Illinois at Urbana-Champaign', 'Bosch Research and Technology Center', 'Carnegie Mellon University']"
Recovery RL: Safe Reinforcement Learning with Learned Recovery Zones,"Safety remains a central obstacle preventing widespread use of RL in the real world: learning new tasks in uncertain environments requires extensive exploration, but safety requires limiting exploration. We propose Recovery RL, an algorithm which navigates this tradeoff by (1) leveraging offline data to learn about constraint violating zones before policy learning and (2) separating the goals of improving task performance and constraint satisfaction across two policies: a task policy that only optimizes the task reward and a recovery policy that guides the agent to safety when constraint violation is likely. We evaluate Recovery RL on 6 simulation domains, including two contact-rich manipulation tasks and an image-based navigation task, and an image-based obstacle avoidance task on a physical robot. We compare Recovery RL to 5 prior safe RL methods which jointly optimize for task performance and safety via constrained optimization or reward shaping and find that Recovery RL outperforms the next best prior method across all domains. Results suggest that Recovery RL trades off constraint violations and task successes 2 - 20 times more efficiently in simulation domains and 3 times more efficiently in physical experiments. See https://tinyurl.com/rl-recovery for videos and supplementary material.
                           ","['Balakrishna, Ashwin', 'Thananjeyan, Brijen', 'Nair, Suraj', 'Luo, Michael', 'Srinivasan, Krishnan', 'Hwang, Minho', 'Gonzalez, Joseph E.', 'Ibarz, Julian', 'Finn, Chelsea', 'Goldberg, Ken']","['University of California, Berkeley', 'UC Berkeley', 'Stanford University', 'UC Berkeley', 'Stanford University', 'University of California Berkeley', 'UC Berkeley', 'Google Inc', 'Stanford University', 'UC Berkeley']"
Learning Shape Control of Elastoplastic Deformable Linear Objects,"Deformable object manipulation tasks have long been regarded as challenging robotic problems. However, until recently very little work has been done on the subject, with most robotic manipulation methods being developed for rigid objects. Deformable objects are more difficult to model and simulate, which has limited the use of model-free Reinforcement Learning (RL) strategies, due to their need for large amounts of data that can only be satisfied in simulation. This paper proposes a new shape control task for Deformable Linear Objects (DLOs). More notably, we present the first study on the effects of elastoplastic properties on this type of problem. Objects with elastoplasticity such as metal wires, are found in various applications and are challenging to manipulate due to their nonlinear behavior. We first highlight the challenges of solving such a manipulation task from an RL perspective, particularly in defining the reward. Then, based on concepts from differential geometry, we propose an intrinsic shape representation using discrete curvature and torsion. Finally, we show through an empirical study that in order to successfully solve the proposed task using Deep Deterministic Policy Gradient (DDPG), the reward needs to include intrinsic information about the shape of the DLO.
                           ","['Laezza, Rita', 'Karayiannidis, Yiannis']","['Chalmers University of Technology', 'Chalmers University of Technology & KTH Royal Institute of Techn']"
DeepQ Stepper: A Framework for Reactive Dynamic Walking on Uneven Terrain,"Reactive stepping and push recovery for biped robots is often restricted to flat terrains because of the difficulty in computing capture regions for nonlinear dynamic models. In this paper, we address this limitation by proposing a novel 3D reactive stepper, the DeepQ stepper, that can approximately learn the 3D capture regions of both simplified and full robot dynamic models using reinforcement learning, which can then be used to find optimal steps. The stepper can take into account the entire dynamics of the robot, ignored in most reactive steppers, leading to a significant improvement in performance. The DeepQ stepper can handle nonconvex terrain with obstacles, walk on restricted surfaces like stepping stones while tracking different velocities, and recover from external disturbances for a constant low computational cost.
                           ","['Meduri, Avadesh', 'Khadiv, Majid', 'Righetti, Ludovic']","['New York University', 'Max Planck Institute for Intelligent Systems', 'New York University']"
Model-Free Reinforcement Learning for Stochastic Games with Linear Temporal Logic Objectives,"We study synthesis of control strategies from linear temporal logic (LTL) objectives in unknown environments. We model this problem as a turn-based zero-sum stochastic game between the controller and the environment, where the transition probabilities and the model topology are fully unknown. The winning condition for the controller in this game is the satisfaction of the given LTL specification, which can be captured by the acceptance condition of a deterministic Rabin automaton (DRA) directly derived from the LTL specification. We introduce a model-free reinforcement learning (RL) methodology to find a strategy that maximizes the probability of satisfying a given LTL specification when the Rabin condition of the derived DRA has a single accepting pair. We then generalize this approach to any LTL formulas, for which the Rabin accepting condition may have more than one pairs, providing a lower bound on the satisfaction probability. Finally, we show applicability of our RL method on two planning case studies.
                           ","['Bozkurt, Alper Kamil', 'Wang, Yu', 'Zavlanos, Michael M.', 'Pajic, Miroslav']","['Duke University', 'Duke University', 'Duke University', 'Duke University']"
Secure Planning against Stealthy Attacks Via Model-Free Reinforcement Learning,"We consider the problem of security-aware planning in an unknown stochastic environment, in the presence of attacks on control signals (i.e., actuators) of the robot. We model the attacker as an agent who has the full knowledge of the controller as well as the employed intrusion-detection system and who wants to prevent the controller from performing tasks while staying stealthy. We formulate the problem as a stochastic game between the attacker and the controller and present an approach to express the objective of such an agent and the controller as a combined linear temporal logic (LTL) formula. We then show that the planning problem, described formally as the problem of satisfying an LTL formula in a stochastic game, can be solved via model-free reinforcement learning when the environment is completely unknown. Finally, we illustrate and evaluate our methods on two robotic planning case studies.
                           ","['Bozkurt, Alper Kamil', 'Wang, Yu', 'Pajic, Miroslav']","['Duke University', 'Duke University', 'Duke University']"
Deep Reinforcement Learning Framework for Underwater Locomotion of Soft Robot,"Soft robotics is an emerging technology with excellent application prospects. However, due to the inherent compliance of the materials used to build soft robots, it is extremely complicated to control soft robots accurately. In this paper, we introduce a data-based control framework for solving the soft robot underwater locomotion problem using deep reinforcement learning (DRL). We first built a soft robot that can swim based on the dielectric elastomer actuator (DEA). We then modeled it in a simulation for the purpose of training the neural network and tested the performance of the control framework through real experiments on the robot. The framework includes the following: a simulation method for the soft robot that can be used to collect data for training the neural network, the neural network controller of the swimming robot trained in the simulation environment. We confirmed the effectiveness of the learning method for the soft swimming robot in the simulation environment by allowing the robot to learn how to move from a random initial state to a specific direction. After obtaining the trained neural network through the simulation, we deployed it on the real robot and tested the performance of the control framework. The soft robot successfully achieved the goal of moving in a straight line in disturbed water. The experimental results suggest the potential of using deep reinforcement learning to improve the locomotion ability of mobile soft robots.
                           ","['Li, Guanda', 'Shintake, Jun', 'Hayashibe, Mitsuhiro']","['Tohoku University', 'University of Electro-Communications', 'Tohoku University']"
Quantification of Joint Redundancy Considering Dynamic Feasibility Using Deep Reinforcement Learning,"The robotic joint redundancy for executing a task and the optimal usage of robotic joints given the redundant degrees of freedom are crucial for the performance of a robot. It is therefore of interest to quantify the joint redundancy to better understand the robotic dexterity considering the dynamic feasibility. To this end, model-based approaches have been among the most commonly used methods to quantify the joint redundancy of simple robots analytically. However, this classical approach fails when applied to non-conventional complex robots. In this study, we propose a new method based on a deep reinforcement learning-derived metric, the synergy exploration area (SEA) metric, for the quantification of redundancy with a given dynamic environment. We conducted various experiments with different robotic structures for different tasks, ranging from simple robotic arm manipulation to more complex robotic locomotion. The experimental results show that the SEA metric can effectively quantify the relative joint redundancy over different robotic structures with varying degrees of freedom under unknown dynamic situations. We further demonstrate the possibility of using the SEA metric to evaluate the optimality of a robotic structure for a given task.
                           ","['Chai, Jiazheng', 'Hayashibe, Mitsuhiro']","['Tohoku University', 'Tohoku University']"
Data-Efficient Learning for Complex and Real-Time Physical Problem Solving Using Augmented Simulation,"Humans quickly solve tasks in novel systems with complex dynamics, without requiring much interaction. While deep reinforcement learning algorithms have achieved tremendous success in many complex tasks, these algorithms need a large number of samples to learn meaningful policies. In this paper, we present a task for navigating a marble to the center of a circular maze. While this system is very intuitive and easy for humans to solve, it can be very difficult and inefficient for standard reinforcement learning algorithms to learn meaningful policies. We present a model that learns to move a marble in the complex environment within minutes of interacting with the real system. Learning consists of initializing a physics engine with parameters estimated using data from the real system. The error in the physics engine is then corrected using Gaussian process regression, which is used to model the residual between real observations and physics engine simulations. The physics engine augmented with the residual model is then used to control the marble in the maze environment using a model-predictive feedback over a receding horizon. To the best of our knowledge, this is the first time that a hybrid model consisting of a full physics engine along with a statistical function approximator has been used to control a complex physical system in real-time using nonlinear model-predictive control (NMPC).
                           ","['Ota, Kei', 'Jha, Devesh', 'Romeres, Diego', 'Vanbaar, Jeroen', 'Smith, Kevin', 'Semitsu, Takayuki', 'Oiki, Tomohiro', 'Sullivan, Alan', 'Nikovski, Daniel', 'Tenenbaum, Joshua']","['Mitsubishi Electric', 'Mitsubishi Electric Research Laboratories', 'Mitsubishi Electric Research Laboratories', 'MERL', 'Massachusetts Institute of Technology', 'Mitsubishi Electric', 'Mitsubishi Electric', 'Mitsubishi Electric Research Lab', 'MERL', 'Massachusetts Institute of Technology']"
Dynamics Randomization Revisited: A Case Study for Quadrupedal Locomotion,"Understanding the gap between simulation and reality is critical for reinforcement learning with legged robots, which are largely trained in simulation. However, recent work has resulted in sometimes conflicting conclusions with regard to which factors are important for success, including the role of dynamics randomization. In this paper, we aim to provide clarity and understanding on the role of dynamics randomization in learning robust locomotion policies for the Laikago quadruped robot. Surprisingly, in contrast to prior work with the same robot model, we find that direct sim-to-real transfer is possible without dynamics randomization or on-robot adaptation schemes. We conduct extensive ablation studies in a sim-to-sim setting to understand the key issues underlying successful policy transfer, including other design decisions that can impact policy robustness. We further ground our conclusions via sim-to-real experiments with various gaits, speeds, and stepping frequencies.
                           ","['Xie, Zhaoming', 'Da, Xingye', 'van de Panne, Michiel', 'Babich, Buck', 'Garg, Animesh']","['University of British Columbia', 'University of Michigan, Ann Arbor', 'University of British Columbia', 'NVIDIA', 'University of Toronto']"
Context-Aware Safe Reinforcement Learning for Non-Stationary Environments,"Safety is a critical concern when deploying reinforcement learning agents for realistic tasks. Recently, safe reinforcement learning algorithms have been developed to optimize the agent's performance while avoiding violations of safety constraints. However, few studies have addressed the non-stationary disturbances in the environments, which may cause catastrophic outcomes. In this paper, we propose the context-aware safe reinforcement learning (CASRL) method, a meta-learning framework to realize safe adaptation in non-stationary environments. We use a probabilistic latent variable model to achieve fast inference of the posterior environment transition distribution given the context data. Safety constraints are then evaluated with uncertainty-aware trajectory sampling. Prior safety constraints are formulated with domain knowledge to improve safety during exploration. The algorithm is evaluated in realistic safety-critical environments with non-stationary disturbances. Results show that the proposed algorithm significantly outperforms existing baselines in terms of safety and robustness.
                           ","['Chen, Baiming', 'Liu, Zuxin', 'Zhu, Jiacheng', 'Xu, Mengdi', 'Ding, Wenhao', 'Li, Liang', 'Zhao, Ding']","['Tsinghua University', 'Carnegie Mellon University', 'Carnegie Mellon University', 'Carnegie Mellon University', 'Carnegie Mellon University', 'Tsinghua University', 'Carnegie Mellon University']"
Decision Making for Autonomous Driving Via Augmented Adversarial Inverse Reinforcement Learning,"Making decisions in complex driving environments is a challenging task for autonomous agents. Imitation learning methods have great potentials for achieving such a goal. Adversarial Inverse Reinforcement Learning (AIRL) is one of the state-of-art imitation learning methods that can learn both a behavioral policy and a reward function simultaneously, yet it is only demonstrated in simple and static environments where no interactions are introduced. In this paper, we improve and stabilize AIRL's performance by augmenting it with semantic rewards in the learning framework. Additionally, we adapt the augmented AIRL to a more practical and challenging decision-making task in a highly interactive environment in autonomous driving. The proposed method is compared with four baselines and evaluated by four performance metrics. Simulation results show that the augmented AIRL outperforms all the baseline methods, and its performance is comparable with that of the experts on all of the four metrics.
                           ","['Wang, Pin', 'Liu, Dapeng', 'Chen, Jiayu', 'Li, Hanhan', 'Chan, Ching-Yao']","['University of California, Berkeley', 'Chalmers University of Technology, Zenseact AB', 'Peking University', 'University of California, Berkeley', 'Univerisity of California, Berkeley']"
ReForm: A Robot Learning Sandbox for Deformable Linear Object Manipulation,"Recent advances in machine learning have triggered an enormous interest in using learning-based approaches for robot control and object manipulation. While the majority of existing algorithms are evaluated under the assumption that the involved bodies are rigid, a large number of practical applications contain deformable objects. In this work we focus on Deformable Linear Objects (DLOs) which can be used to model cables, tubes or wires. They are present in many applications such as manufacturing, agriculture and medicine. New methods in robotic manipulation research are often demonstrated in custom environments impeding reproducibility and comparisons of algorithms. We introduce ReForm, a simulation sandbox and a tool for benchmarking manipulation of DLOs. We offer six distinct environments representing important characteristics of deformable objects such as elasticity, plasticity or self-collisions and occlusions. A modular framework is used, enabling design parameters such as the end-effector degrees of freedom, reward function and type of observation. ReForm is a novel robot learning sandbox with which we intend to facilitate testing and reproducibility in manipulation research for DLOs.
                           ","['Laezza, Rita', 'Gieselmann, Robert', 'Pokorny, Florian T.', 'Karayiannidis, Yiannis']","['Chalmers University of Technology', 'KTH Royal Institute of Technology', 'KTH Royal Institute of Technology', 'Chalmers University of Technology & KTH Royal Institute of Techn']"
GoSafe: Globally Optimal Safe Robot Learning,"When learning policies for robotic systems from data, safety is a major concern, as violation of safety constraints may cause hardware damage. SafeOpt is an efficient Bayesian optimization (BO) algorithm that can learn policies while guaranteeing safety with high probability. However, its search space is limited to an initially given safe region. We extend this method by exploring outside the initial safe area while still guaranteeing safety with high probability. This is achieved by learning a set of initial conditions from which we can recover safely using a learned backup controller in case of a potential failure. We derive conditions for guaranteed convergence to the global optimum and validate GoSafe in hardware experiments.
                           ","['Baumann, Dominik', 'Marco, Alonso', 'Turchetta, Matteo', 'Trimpe, Sebastian']","['RWTH Aachen University', 'Max Planck Institute for Intelligent Systems', 'ETH Zurich', 'RWTH Aachen University']"
Learning Functionally Decomposed Hierarchies for Continuous Control Tasks with Path Planning,"We present HiDe, a novel hierarchical reinforcement learning architecture that successfully solves long horizon control tasks and generalizes to unseen test scenarios. Functional decomposition between planning and low-level control is achieved by explicitly separating the state-action spaces across the hierarchy, which allows the integration of task-relevant knowledge per layer. We propose an RL-based planner to efficiently leverage the information in the planning layer of the hierarchy, while the control layer learns a goal-conditioned control policy. The hierarchy is trained jointly but allows for the modular transfer of policy layers across hierarchies of different agents. We experimentally show that our method generalizes across unseen test environments and can scale to 3x horizon length compared to both learning and non-learning based methods. We evaluate on complex continuous control tasks with sparse rewards, including navigation and robot manipulation.
                           ","['Christen, Sammy', 'Jendele, Lukas', 'Aksan, Emre', 'Hilliges, Otmar']","['ETH Zurich', 'ETH Zurich', 'ETHZ', 'ETH Zurich']"
Super-Human Performance in Gran Turismo Sport Using Deep Reinforcement Learning,"Autonomous car racing is a major challenge in robotics. It raises fundamental problems for classical approaches such as planning minimum-time trajectories under uncertain dynamics and controlling the car at the limits of its handling. Besides, the requirement of minimizing the lap time, which is a sparse objective, and the difficulty of collecting training data from human experts have also hindered researchers from directly applying learning-based approaches to solve the problem. In the present work, we propose a learning-based system for autonomous car racing by leveraging a high-fidelity physical car simulation, a course-progress proxy reward, and deep reinforcement learning. We deploy our system in Gran Turismo Sport, a world-leading car simulator known for its realistic physics simulation of different race cars and tracks, which is even used to recruit human race car drivers. Our trained policy achieves autonomous racing performance that goes beyond what had been achieved so far by the built-in AI, and, at the same time, outperforms the fastest driver in a dataset of over 50,000 human players.
                           ","['Fuchs, Florian', 'Song, Yunlong', 'Kaufmann, Elia', 'Scaramuzza, Davide', 'Duerr, Peter']","['Sony', 'University of Zurich', 'University of Zurich', 'University of Zurich', 'Sony']"
Spatial Intention Maps for Multi-Agent Mobile Manipulation,"The ability to communicate intention enables decentralized multi-agent robots to collaborate while performing physical tasks. In this work, we present spatial intention maps, a new intention representation for multi-agent vision-based deep reinforcement learning that improves coordination between decentralized mobile manipulators. In this representation, each agent's intention is provided to other agents, and rendered into an overhead 2D map aligned with visual observations. This synergizes with the recently proposed spatial action maps framework, in which state and action representations are spatially aligned, providing inductive biases that encourage emergent cooperative behaviors requiring spatial coordination, such as passing objects to each other or avoiding collisions. Experiments across a variety of multi-agent environments, including heterogeneous robot teams with different abilities (lifting, pushing, or throwing), show that incorporating spatial intention maps improves performance for different mobile manipulation tasks while significantly enhancing cooperative behaviors.
                           ","['Wu, Jimmy', 'Sun, Xingyuan', 'Zeng, Andy', 'Song, Shuran', 'Rusinkiewicz, Szymon', 'Funkhouser, Thomas A.']","['Princeton University', 'Princeton University', 'Google', 'Columbia University', 'Princeton University', 'Princeton University']"
A Safe Hierarchical Planning Framework for Complex Driving Scenarios Based on Reinforcement Learning,"Autonomous vehicles need to handle various traffic conditions and make safe and efficient decisions and maneuvers. However, on the one hand, a single optimization/sampling-based motion planner cannot efficiently generate safe trajectories in real time, particularly when there are many interactive vehicles near by. On the other hand, end-to-end learning methods cannot assure the safety of the outcomes. To address this challenge, we propose a hierarchical behavior planning framework with a set of low-level safe controllers and a high-level reinforcement learning algorithm (H-CtRL) as a coordinator for the low-level controllers. Safety is guaranteed by the low-level optimization/sampling-based controllers, while the high-level reinforcement learning algorithm makes H-CtRL an adaptive and efficient behavior planner. To train and test our proposed algorithm, we built a simulator that can reproduce traffic scenes using real-world datasets. The proposed H-CtRL is proved to be effective in various realistic simulation scenarios, with satisfying performance in terms of both safety and efficiency.
                           ","['Li, Jinning', 'Sun, Liting', 'Chen, Jianyu', 'Tomizuka, Masayoshi', 'Zhan, Wei']","['University of California, Berkeley', 'University of California, Berkeley', 'Tsinghua University', 'University of California', 'Univeristy of California, Berkeley']"
Batch Exploration with Examples for Scalable Robotic Reinforcement Learning,"Learning from diverse offline datasets is a promising path towards learning general purpose robotic agents. However, a core challenge in this paradigm lies in collecting large amounts of meaningful data, while not depending on a human in the loop for data collection. One way to address this challenge is through task-agnostic exploration, where an agent attempts to explore without a task-specific reward function, and collect data that can be useful for any subsequent task. While these approaches have shown some promise in simple domains, they often struggle to explore the relevant regions of the state space in more challenging settings, such as vision-based robotic manipulation. This challenge stems from an objective that encourages exploring everything in a potentially vast state space. To mitigate this challenge, we propose to focus exploration on the important parts of the state space using weak human supervision. Concretely, we propose an exploration technique, Batch Exploration with Examples (BEE), that explores relevant regions of the state space, guided by a modest number of human-provided images of important states. We find that BEE is able to tackle challenging vision-based manipulation tasks both in simulation and on a real Franka Emika Panda robot, and observe that compared to task-agnostic and weakly-supervised exploration techniques, it (1) interacts more than twice as often with relevant objects, and (2) improves task performance when used with offline RL.
                           ","['Chen, Annie', 'Nam, HyunJi', 'Nair, Suraj', 'Finn, Chelsea']","['Stanford University', 'Stanford University', 'Stanford University', 'Stanford University']"
A Scavenger Hunt for Service Robots,"Creating robots that can perform general-purpose service tasks in a human-populated environment has been a longstanding grand challenge for AI and Robotics research. One particularly valuable skill that is relevant to a wide variety of tasks, is the ability to locate and retrieve objects upon request. 		 This paper models this skill as a Scavenger Hunt (SH) game, which we formulate as a variation of the NP-hard stochastic traveling purchaser problem. In this problem, the goal is to find a set of objects as quickly as possible, given probability distributions of where they may be found. 		 We investigate the performance of several solution algorithms for the SH problem, both in simulation and on a real mobile robot. We use Reinforcement Learning (RL) to train an agent to plan a minimal cost path, and show that the RL agent can outperform a range of heuristic algorithms, achieving near optimal performance. 		 In order to stimulate research on this problem, we introduce a publicly available software stack and associated website that enable users to upload scavenger hunts which robots can download, perform, and learn from to continually improve their performance on future hunts.
                           ","['Yedidsion, Harel', 'Suriadinata, Jennifer', 'Xu, Zifan', 'deBruyn, Stefan', 'Stone, Peter']","['University of Texas at Austin', 'University of Texas at Austin', 'University of Texas at Austin', 'The University of Texas at Austin', 'University of Texas at Austin']"
Reward Machines for Vision-Based Robotic Manipulation,"Deep Q learning (DQN) has enabled robot agents to accomplish vision based tasks that seemed out of reach. Despite recent success stories, there are still several sources of computational complexity that challenge the performance of DQN. We place the focus on vision manipulation tasks, where the correct action selection is often predicated on a small number of pixels. We observe that in some of these tasks DQN does not converge to the optimal Q function, and their values do not separate well optimal and suboptimal actions. In consequence, the policies obtained with DQN tend to be brittle and manifest a low success rate, especially in long horizon tasks. In this work we show the benefits of Reward Machines (RMs) for Deep Q learning (DQRM) in vision based robot manipulation tasks. Reward machines decompose the task at an abstract level, inform the agent about their current stage along task completion, and guide them via dense rewards. We show that RMs help DQN learn the optimal Q values in each abstract state. Their policies are more robust, manifest higher success rate, and are learned with fewer training steps compared with DQN. The benefits of RMs are more evident in long-horizon tasks, where we show that DQRM is able to learn good-quality policies with six times times fewer training steps than DQN, even when this is equipped with dense reward shaping.
                           ","['Camacho, Alberto', 'Varley, Jacob', 'Zeng, Andy', 'Jain, Deepali', 'Iscen, Atil', 'Kalashnikov, Dmitry']","['Google', 'Google', 'Google', 'Robotics at Google', 'Google', 'Google Brain']"
Tactile-RL for Insertion: Generalization to Objects of Unknown Geometry,"Object insertion is a classic contact-rich manipulation task. The task remains challenging, especially when considering general objects of unknown geometry, which significantly limits the ability to understand the contact configuration between the object and the environment. We study the problem of aligning the object and environment with a tactile-based feedback insertion policy. The insertion process is modeled as an episodic policy that iterates between insertion attempts followed by pose corrections. We explore different mechanisms to learn such a policy based on Reinforcement Learning. The key contribution of this paper is to demonstrate that it is possible to learn a tactile insertion policy that generalizes across different object geometries, and an ablation study of the key design choices for the learning agent: 1) the type of learning scheme: supervised vs. reinforcement learning; 2) the type of learning schedule: unguided vs. curriculum learning; 3) the type of sensing modality: force/torque vs. tactile; and 4) the type of tactile representation: tactile RGB vs. tactile flow. We show that the optimal configuration of the learning agent (RL + curriculum + tactile flow) exposed to 4 training objects yields an insertion policy that inserts 4 novel objects with over 85% success rate and within 3~4 attempts. Comparisons between F/T and tactile sensing show that while an F/T-based policy learns more efficiently, a tactile-based policy provides better generalization.
                           ","['Dong, Siyuan', 'Jha, Devesh', 'Romeres, Diego', 'Kim, Sangwoon', 'Nikovski, Daniel', 'Rodriguez, Alberto']","['MIT', 'Mitsubishi Electric Research Laboratories', 'Mitsubishi Electric Research Laboratories', 'Massachusetts Institute of Technology', 'MERL', 'Massachusetts Institute of Technology']"
Meta-Adversarial Inverse Reinforcement Learning for Decision-Making Tasks,"Learning from demonstrations has made great progress over the past few years. However, it is generally data hungry and task specific. In other words, it requires a large amount of data to train a decent model on a particular task, and the model often fails to generalize to new tasks that have a different distribution. In practice, demonstrations from new tasks will be continuously observed and the data might be unlabeled or only partially labeled. Therefore, it is desirable for the trained model to adapt to new tasks that have limited data samples available. In this work, we build an adaptable imitation learning model based on the integration of Meta-learning and Adversarial Inverse Reinforcement Learning(Meta-AIRL). We exploit the adversarial learning and inverse reinforcement learning mechanisms to learn policies and reward functions simultaneously from available training tasks and then adapt them to new tasks with the meta-learning framework. Simulation results show that the adapted policy trained with Meta-AIRL can effectively learn from limited number of demonstrations, and quickly reach the performance comparable to that of the experts on unseen tasks.
                           ","['Wang, Pin', 'Li, Hanhan', 'Chan, Ching-Yao']","['University of California, Berkeley', 'University of California, Berkeley', 'Univerisity of California, Berkeley']"
APPLR: Adaptive Planner Parameter Learning from Reinforcement,"Classical navigation systems typically operate using a fixed set of hand-picked parameters (e.g. maximum speed, sampling rate, inflation radius, etc.) and require heavy expert re-tuning in order to work in new environments. To mitigate this requirement, it has been proposed to learn parameters for different contexts in a new environment using human demonstrations collected via teleoperation. However, learning from human demonstration limits deployment to the training environment, and limits overall performance to that of a potentially-suboptimal demonstrator. In this paper, we introduce APPLR, Adaptive Planner Parameter Learning from Reinforcement, which allows existing navigation systems to adapt to new scenarios by using a parameter selection scheme discovered via reinforcement learning (RL) in a wide variety of simulation environments. We evaluate APPLR on a robot in both simulated and physical experiments, and show that it can outperform both a fixed set of hand-tuned parameters and also a dynamic parameter tuning scheme learned from human demonstration.
                           ","['Xu, Zifan', 'Dhamankar, Gauraang', 'Nair, Anirudh', 'Xiao, Xuesu', 'Warnell, Garrett', 'Liu, Bo', 'Wang, Zizhao', 'Stone, Peter']","['University of Texas at Austin', 'The University of Texas at Austin', 'The University of Texas at Austin', 'The University of Texas at Austin', 'U.S. Army Research Laboratory', 'University of Texas at Austin', 'University of Texas - Austin', 'University of Texas at Austin']"
Behavior Planning at Urban Intersections through Hierarchical Reinforcement Learning,"For autonomous vehicles, effective behavior planning is crucial to ensure safety of the ego car. In many urban scenarios, it is hard to create sufficiently general heuristic rules, especially for challenging scenarios that some new human drivers find difficult. In this work, we propose a behavior planning structure based on reinforcement learning (RL) which is capable of performing autonomous vehicle behavior planning with a hierarchical structure in simulated urban environments. Application of the hierarchical structure allows the various layers of the behavior planning system to be satisfied. Our algorithms can perform better than heuristic-rule-based methods for elective decisions such as when to turn left between vehicles approaching from the opposite direction or possible lane-change when approaching an intersection due to lane blockage or delay in front of the ego car. Such behavior is hard to evaluate as correct or incorrect, but for some aggressive expert human drivers handle such scenarios effectively and quickly. On the other hand, compared to traditional RL methods, our algorithm is more sample-efficient, due to the use of a hybrid reward mechanism and heuristic exploration during the training process. The results also show that the proposed method converges to an optimal policy faster than traditional RL methods.
                           ","['Qiao, Zhiqian', 'Schneider, Jeff', 'Dolan, John M.']","['Carnegie Mellon University', 'Carnegie Mellon University', 'Carnegie Mellon University']"
Scalable Learning of Safety Guarantees for Autonomous Systems Using Hamilton-Jacobi Reachability,"Autonomous systems like aircraft and assistive robots often operate in scenarios where guaranteeing safety is critical. Methods like Hamilton-Jacobi reachability can provide guaranteed safe sets and controllers for such systems. However, often these same scenarios have unknown or uncertain environments, system dynamics, or predictions of other agents. As the system is operating, it may learn new knowledge about these uncertainties and should therefore update its safety analysis accordingly. However, work to learn and update safety analysis is limited to small systems of about two dimensions due to the computational complexity of the analysis. In this paper we synthesize several techniques to speed up computation: decomposition, warm-starting, and adaptive grids. Using this new framework we can update safe sets by one or more orders of magnitude faster than prior work, making this technique practical for many realistic systems. We demonstrate our results on simulated 2D and 10D near-hover quadcopters operating in a windy environment.
                           ","['Herbert, Sylvia', 'Choi, Jason J.', 'Sanjeev, Suvansh', 'Gibson, Marsalis', 'Sreenath, Koushil', 'Tomlin, Claire']","['UC San Diego (UCSD)', 'University of California, Berkeley', 'Carnegie Mellon University', 'University of California - Berkeley', 'University of California, Berkeley', 'UC Berkeley']"
Learning to Robustly Negotiate Bi-Directional Lane Usage in High-Conflict Driving Scenarios,"Recently, autonomous driving has made substantial progress in addressing the most common traffic scenarios like intersection navigation and lane changing. However, most of these successes have been limited to scenarios with well-defined traffic rules and require minimal negotiation with other vehicles. In this paper, we introduce a previously unconsidered, yet everyday, high-conflict driving scenario requiring negotiations between agents of equal rights and priorities. There exists no centralized control structure and we do not allow communications. Therefore, it is unknown if other drivers are willing to cooperate, and if so to what extent. We train policies to robustly negotiate with opposing vehicles of an unobservable degree of cooperativeness using multi-agent reinforcement learning (MARL). We propose Discrete Asymmetric Soft Actor-Critic (DASAC), a maximum- entropy off-policy MARL algorithm allowing for centralized training with decentralized execution. We show that using DASAC we are able to successfully negotiate and traverse the scenario considered over 99% of the time. Our agents are robust to an unknown timing of opponent decisions, an unobservable degree of cooperativeness of the opposing vehicle, and previously unencountered policies. Furthermore, they learn to exhibit human-like behaviors such as defensive driving, anticipating solution options and interpreting the behavior of other agents.
                           ","['Killing, Christoph', 'Villaflor, Adam', 'Dolan, John M.']","['Technical University of Munich', 'CMU', 'Carnegie Mellon University']"
Learning Dense Rewards for Contact-Rich Manipulation Tasks,"Rewards play a crucial role in reinforcement learning. To arrive at the desired policy, the design of a suitable reward function often requires signification domain expertise as well as trial-and-error. Here, we aim to minimize the effort involved in designing reward functions for contact-rich manipulation tasks. In particular, we provide an approach capable of extracting dense reward functions algorithmically from robots¡¯ high-dimensional observations, such as images and tactile feedback. In contrast to state-of-the-art high-dimensional reward learning methodologies, our approach does not leverage adversarial training, and is thus less prone to the associated training instabilities. Instead, our approach learns rewards by estimating task progress in a self-supervised manner. We demonstrate the effectiveness and efficiency of our approach on two contact-rich manipulation tasks, namely, peg-in-hole and USB insertion. The experimental results indicate that the policies trained with the learned reward function achieves better performance and faster convergence compared to the baselines.
                           ","['Wu, Zheng', 'Lian, Wenzhao', 'Unhelkar, Vaibhav V.', 'Tomizuka, Masayoshi', 'Schaal, Stefan']","['University of California, Berkeley', 'Google X', 'Rice University', 'University of California', 'Google X']"
Improved Learning of Robot Manipulation Tasks Via Tactile Intrinsic Motivation,"In this paper we address the challenge of exploration in deep reinforcement learning for robotic manipulation tasks. In sparse goal settings, an agent does not receive any positive feedback until randomly achieving the goal, which becomes infeasible for longer control sequences. Inspired by touch-based exploration observed in children, we formulate an intrinsic reward based on the sum of forces between a robot's force sensors and manipulation objects that encourages physical interaction. Furthermore, we introduce contact-prioritized experience replay, a sampling scheme that prioritizes contact rich episodes and transitions. We show that our solution accelerates the exploration and outperforms state-of-the-art methods on three fundamental robot manipulation benchmarks.
                           ","['Vulin, Nikola', 'Christen, Sammy', 'Stevsic, Stefan', 'Hilliges, Otmar']","['ETH Zurich', 'ETH Zurich', 'ETH Zurich', 'ETH Zurich']"
Robot Learning with Crash Constraints,"In the past decade, numerous machine learning algorithms have been shown to successfully learn optimal policies to control real robotic systems. However, it is common to encounter failing behaviors as the learning loop progresses. Specifically, in robot applications where failing is undesired but not catastrophic, many algorithms struggle with leveraging data obtained from failures. This is usually caused by (i) the failed experiment ending prematurely, or (ii) the acquired data being scarce or corrupted. Both complicate the design of proper reward functions to penalize failures. In this paper, we propose a framework that addresses those issues. We consider failing behaviors as those that violate a constraint and address the problem of learning with crash constraints, where no data is obtained upon constraint violation. The no-data case is addressed by a novel GP model (GPCR) for the constraint that combines discrete events (failure/success) with continuous observations (only obtained upon success). We demonstrate the effectiveness of our framework on simulated benchmarks and on a real jumping quadruped, where the constraint threshold is unknown a priori. Experimental data is collected, by means of constrained Bayesian optimization, directly on the real robot. Our results outperform manual tuning and GPCR proves useful on estimating the constraint threshold.
                           ","['Marco, Alonso', 'Baumann, Dominik', 'Khadiv, Majid', 'Hennig, Philipp', 'Righetti, Ludovic', 'Trimpe, Sebastian']","['Max Planck Institute for Intelligent Systems', 'RWTH Aachen University', 'Max Planck Institute for Intelligent Systems', 'MPI Intelligent Systems', 'New York University', 'RWTH Aachen University']"
Autonomous Overtaking in Gran Turismo Sport Using Curriculum Reinforcement Learning,"Professional race-car drivers can execute extreme overtaking maneuvers. However, existing algorithms for autonomous overtaking either rely on simplified assumptions about the vehicle dynamics or try to solve expensive trajectory-optimization problems online. When the vehicle approaches its physical limits, existing model-based controllers struggle to handle highly nonlinear dynamics, and cannot leverage the large volume of data generated by simulation or real-world driving. To circumvent these limitations, we propose a new learning-based method to tackle the autonomous overtaking problem. We evaluate our approach in the popular car racing game Gran Turismo Sport, which is known for its detailed modeling of various cars and tracks. By leveraging curriculum learning, our approach leads to faster convergence as well as increased performance compared to vanilla reinforcement learning. % As a result, the trained controller outperforms the built-in model-based game AI and achieves comparable overtaking performance with an experienced human driver.
                           ","['Song, Yunlong', 'Lin, HaoChih', 'Kaufmann, Elia', 'Duerr, Peter', 'Scaramuzza, Davide']","['University of Zurich', 'ETH Zurich', 'University of Zurich', 'Sony', 'University of Zurich']"
Reachability-Based Trajectory Safeguard (RTS): A Safe and Fast Reinforcement Learning Safety Layer for Continuous Control,"Reinforcement Learning (RL) algorithms have achieved remarkable performance in decision making and control tasks due to their ability to reason about long-term, cumulative reward using trial and error. However, during RL training, applying this trial-and-error approach to real-world robots operating in safety critical environment may lead to collisions. To address this challenge, this paper proposes a Reachability-based Trajectory Safeguard (RTS), which leverages reachability analysis to ensure safety during training and operation. Given a known (but uncertain) model of a robot, RTS precomputes a Forward Reachable Set of the robot tracking a continuum of parameterized trajectories. At runtime, the RL agent selects from this continuum in a receding-horizon way to control the robot; the FRS is used to identify if the agent's choice is safe or not, and to adjust unsafe choices. The efficacy of this method is illustrated on three nonlinear robot models, including a 12-D quadrotor drone, in simulation and in comparison with state-of-the-art safe motion planning methods.
                           ","['Shao, Yifei', 'Chen, Chao', 'Kousik, Shreyas', 'Vasudevan, Ram']","['University of Michigan-Ann Arbor', 'University of Michigan - Ann Arbor', 'Stanford University', 'University of Michigan']"
Reaching Pruning Locations in a Vine Using a Deep Reinforcement Learning Policy,"We outline a neural network-based pipeline for perception, control and planning of a 7 DoF robot for tasks that involve reaching into a dormant grapevine canopy. The proposed system consists of a 6 DoF industrial robot arm and a linear slider that can actuate on an entire grape vine. Our approach uses Convolutional Neural Networks to detect buds in dormant grape vines and a Reinforcement Learning based control strategy to reach desired cut-point locations for pruning tasks. Within this framework, three methodologies are developed and compared to reach the desired locations: the learned policy-based approach (RL), a hybrid method that uses the learned policy and an inverse kinematics solver (RL+IK), and lastly a classical approach commonly used in robotics. We first tested and validated the suitability of the proposed learning methodology in a simulated environment that resembled laboratory conditions. A reaching accuracy of up to 61.90% and 85.71% for the RL and RL+IK approaches respectively was obtained for a vine that the agent observed while learning. When testing in a new vine, the accuracy was up to 66.66% and 76.19% for RL and RL+IK, respectively. The same methods were then deployed on a real system in an end to end procedure: autonomously scan the vine using a vision system, create its model and finally use the learned policy to reach cutting points. The reaching accuracy obtained in these tests was 73.08%.
                           ","['Yandun, Francisco', 'Parhar, Tanvir', 'Silwal, Abhisesh', 'Clifford, David', 'Yuan, Zhiqiang', 'Levine, Gabriella', 'Yaroshenko, Sergey', 'Kantor, George']","['Carnegie Mellon University', 'Carnegie Mellon University', 'Carnegie Mellon University', 'Mineral, X - the Moonshot Factory', 'Mineral, X - the Moonshot Factory', 'X Company', 'X Development Company', 'Carnegie Mellon University']"
Mesh Based Analysis of Low Fractal Dimension Reinforcement Learning Policies,"In previous work, using a process we call meshing, the reachable state spaces for various continuous and hybrid systems were approximated as a discrete set of states which can then be synthesized into a Markov chain. One of the applications for this approach has been to analyze locomotion policies obtained by reinforcement learning, in a step towards making empirical guarantees about the stability properties of the resulting system. In a separate line of research, we introduced a modified reward function for on-policy reinforcement learning algorithms that utilizes a ""fractal dimension"" of rollout trajectories. This reward was shown to encourage policies that induce individual trajectories which can be more compactly represented as a discrete mesh. In this work, we combine these two threads of research by building meshes of the reachable state space of a system subject to disturbances and controlled by policies obtained with the modified reward. Our analysis shows that the modified policies do produce much smaller reachable meshes. This shows that agents trained with the fractal dimension reward transfer their desirable quality of having a more compact state space to a setting with external disturbances. The results also suggest that the previous work using mesh based tools to analyze RL policies may be extended to higher dimensional systems or to higher resolution meshes than would have otherwise been possible.
                           ","['Gillen, Sean', 'Byl, Katie']","['University of California Santa Barbara', 'UCSB']"
"Learning from Simulation, Racing in Reality","We present a reinforcement learning-based solution to autonomously race on a miniature race car platform. We show that a policy that is trained purely in simulation using a relatively simple vehicle model, including model randomization, can be successfully transferred to the real robotic setup. We achieve this by using novel policy output regularization approach and a lifted action space which enables smooth actions but still aggressive race car driving. We show that this regularized policy does outperform the Soft Actor Critic (SAC) baseline method, both in simulation and on the real car, but it is still outperformed by a Model Predictive Controller (MPC) state of the art method. The refinement of the policy with three hours of real-world interaction data allows the reinforcement learning policy to achieve lap times similar to the MPC controller while reducing track constraint violations by 50%.
                           ","['Chisari, Eugenio', 'Liniger, Alexander', 'Rupenyan, Alisa', 'Van Gool, Luc', 'Lygeros, John']","['Albert-Ludwigs-Universit?t Freiburg', 'ETH Zurich', 'ETH Z¨¹rich', 'ETH Zurich', 'ETH Zurich']"
Robot in a China Shop: Using Reinforcement Learning for Location-Specific Navigation Behaviour,"Robots need to be able to work in multiple different environments. Even when performing similar tasks, different behaviour should be deployed to best fit the current environment. In this paper, We propose a new approach to navigation, where it is treated as a multi-task learning problem. This enables the robot to learn to behave differently in visual navigation tasks for different environments while also learning shared expertise across environments. We evaluated our approach in both simulated environments as well as real-world data. Our method allows our system to converge with a 26% reduction in training time, while also increasing accuracy.
                           ","['Bian, Xihan', 'Mendez Maldonado, Oscar Alejandro', 'Hadfield, Simon']","['University of Surrey', 'University of Surrey', 'University of Surrey']"
Real-Time Trajectory Adaptation for Quadrupedal Locomotion Using Deep Reinforcement Learning,"We present a control architecture for real-time adaptation and tracking of trajectories generated using a terrain-aware trajectory optimization solver. This approach enables us to circumvent the computationally exhaustive task of online trajectory optimization, and further introduces a control solution robust to systems modeled with approximated dynamics. We train a policy using deep reinforcement learning (RL) to introduce additive deviations to a reference trajectory in order to generate a feedback-based trajectory tracking system for a quadrupedal robot. We train this policy across a multitude of simulated terrains and ensure its generality by introducing training methods that avoid overfitting and convergence towards local optima. Additionally, in order to capture terrain information, we include a latent representation of the height maps in the observation space of the RL environment as a form of exteroceptive feedback. We test the performance of our trained policy by tracking the corrected set points using a model-based whole-body controller and compare it with the tracking behavior obtained without the corrective feedback in several simulation environments, and show that introducing the corrective feedback results in increase of the success rate from 72.7% to 92.4% for tracking precomputed dynamic long horizon trajectories on flat terrain and from 47.5% to 80.3% on a complex modular uneven terrain.
                           ","['Gangapurwala, Siddhant', 'Geisert, Mathieu', 'Orsolino, Romeo', 'Fallon, Maurice', 'Havoutis, Ioannis']","['University of Oxford', 'University of Oxford', 'Arrival Ltd', 'University of Oxford', 'University of Oxford']"
Model-Based Reinforcement Learning with Provable Safety Guarantees Via Control Barrier Functions,"Safety is a critical property in applications including robotics, transportation, and energy. Safety is especially challenging in reinforcement learning (RL) settings, in which uncertainty of the system dynamics may cause safety violations during exploration. Control Barrier Functions (CBFs), which enforce safety by constraining the control actions at each time step, are a promising approach for safety-critical control. This technique has been applied to ensure the safety of model-free RL, however, it has not been integrated into model-based RL. In this paper, we propose Uncertainty-Tolerant Control Barrier Functions (UTCBFs), a new class of CBFs to incorporate model uncertainty and provide provable safety guarantees with desired probability. Furthermore, we introduce an algorithm for model-based RL to guarantee safety by integrating CBFs with gradient-based policy search. Our approach is verified through a numerical study of a cart-pole system and an inverted pendulum system with comparison to state-of-the-art RL algorithms.
                           ","['Zhang, Hongchao', 'Li, Zhouchi', 'Clark, Andrew']","['Worcester Polytechnic Institute', 'Worcester Polytechnic Institute', 'Worcester Polytechnic Institute']"
Learning Collaborative Pushing and Grasping Policies in Dense Clutter,"Robots must reason about pushing and grasping in order to engage in flexible manipulation in cluttered environments. Earlier works on learning pushing and grasping only consider each operation in isolation or are limited to top-down grasping and bin-picking. We train a robot to learn joint planar pushing and 6-degree-of-freedom (6-DoF) grasping policies by self-supervision. Two separate deep neural networks are trained to map from 3D visual observations to actions with a Q-learning framework. With collaborative pushes and expanded grasping action space, our system can deal with cluttered scenes with a wide variety of objects (e.g. grasping a plate from the side after pushing away surrounding obstacles). We compare our system to the state-of-the-art baseline model VPG in simulation and outperform it with 10% higher action efficiency and 20% higher grasp success rate. We then demonstrate our system on a KUKA LBR iiwa arm with a Robotiq 3-finger gripper.
                           ","['Tang, Bingjie', 'Corsaro, Matt', 'Konidaris, George', 'Nikolaidis, Stefanos', 'Tellex, Stefanie']","['University of Southern California', 'Brown University', 'Brown University', 'University of Southern California', 'Brown']"
Learning Bipedal Robot Locomotion from Human Movement,"Teaching an anthropomorphic robot from human example offers the opportunity to impart humanlike qualities on its movement. In this work we present a reinforcement learning based method for teaching a real world bipedal robot to perform movements directly from human motion capture data. Our method seamlessly transitions from training in a simulation environment to executing on a physical robot without requiring any real world training iterations or offline steps. To overcome the disparity in joint configurations between the robot and the motion capture actor, our method incorporates motion re-targeting into the training process. Domain randomization techniques are used to compensate for the differences between the simulated and physical systems. We demonstrate our method on an internally developed humanoid robot with movements ranging from a dynamic walk cycle to complex balancing and waving. Our controller preserves the style imparted by the motion capture data and exhibits graceful failure modes resulting in safe operation for the robot. This work was performed for research purposes only.
                           ","['Taylor, Michael', 'Bashkirov, Sergey', 'Fernandez Rico, Javier', 'Toriyama, Ike', 'Yanagisawa, Hideki', 'Miyada, Naoyuki', 'Ishizuka, Kensaku']","['Sony Interactive Entertainment', 'Sony Interactive Entertainment', 'Cruise', 'Sony Interactive Entertainment', 'Sony Interactive Entertainment', 'Sony Interactive Entertainment', 'Sony Interactive Entertainment']"
Relational Navigation Learning in Continuous Action Space among Crowds,"In this paper, a novel navigation learning method in continuous action space among crowds based on relational graph is proposed which can be directly used to differential-drive mobile robots without any change. More specifically, in order to increase generalization ability across crowd sizes, Graph Convolutional Network (GCN) is adopted to extract relationships between robot and pedestrians and features that are further utilized as inputs of pedestrian state prediction network, actor network, and critic network. To efficiently and safely learn the navigation policy, all networks are pretrained through imitating ORCA which is the state-of-the-art algorithm in crowd navigation, and then a model-based reinforcement learning method which combine the model prediction and clipped advantage-weighted regression is proposed to finetune the networks. Finally, simulation experiments are performed and verify that the proposed learning method performs significantly better than ORCA and the other state-of-the-art reinforcement learning methods.
                           ","['Zhang, Xueyou', 'Xi, Wei', 'Guo, Xian', 'Fang, Yongchun', 'Wang, Bin', 'Liu, Wulong', 'Hao, Jianye']","['Nankai University', 'Nankai University', 'Nankai University', 'Nankai University', 'Noah¡¯s Ark Lab, Huawei', 'Noah¡¯s Ark Lab, Huawei', ""Noah's Ark Lab""]"
A General Approach for the Automation of Hydraulic Excavator Arms Using Reinforcement Learning,"This article presents a general approach to derive an end effector trajectory tracking controller for highly nonlinear hydraulic excavator arms. Rather than requiring an analytical model of the system, we use a neural network model that is trained based on measurements collected during operation of the machine. The data-driven model effectively represents the actuator dynamics including the cylinder-to-joint-space conversion. Requiring only the distances between the individual joints, a simulation is set up to train a control policy using reinforcement learning (RL). The policy outputs pilot stage control commands that can be directly applied to the machine without further fine-tuning. The proposed approach is implemented on a Menzi Muck M545, a 12 t hydraulic excavator, and tested in different task space trajectory tracking scenarios, with and without soil interaction. Compared to a commercial tracking controller, which requires laborious hand tuning by expert engineers, the learned controller shows higher tracking accuracy, indicating that the achieved performance is sufficient for the practical application on construction sites and that the proposed approach opens a new avenue for future machine automation.
                           ","['Egli, Pascal Arturo', 'Hutter, Marco']","['RSL, ETHZ', 'ETH Zurich']"
Remote-Center-Of-Motion Recommendation Toward Brain Needle Intervention Using Deep Reinforcement Learning,"Brain needle intervention is a typical diagnosis and therapy procedure in brain disorders, such as brain tumors and Parkinson¡¯s disease. Preoperative needle path planning is a vital step to guarantee the patient¡¯s safety and reduce lesions. For positioning accuracy in the CT/MRI environment, we have developed a novel needle intervention robot in our previous work. Because the robot is currently designed for the rigid needle, the task of preoperative path-planning is to search for an optimal Remote Center of Motion (RCM) for needle insertion. Therefore, this work proposes an RCM recommendation system using deep reinforcement learning. Considering the robot kinematics, this system takes the following criteria/constraints into consideration: clinical obstacle (blood vessels, tissues) avoidance (COA), mechanically inverse kinematics (MIK) and mechanically less motion (MLM) for the robot. We design a reward function to combine the above three criteria based on their corresponding importance level and utilize proximal policy optimization (PPO) as the main agent of reinforcement learning (RL). RL methods are proved to be competent in searching the RCM, which satisfies the above criteria simultaneously. On the one hand, the results present that RL agents obtain the success rate of finishing the designed task at 93%, which has reached the human level in the tests. On the other hand, the RL agents have the remarkable capability of combining more complex criteria/constraints in future w
                           ","['Gao, Huxin', 'Xiao, Xiao', 'Qiu, Liang', 'Meng, Max Q.-H.', 'King, Nicolas, Kon Kam', 'Ren, Hongliang']","['National University of Singapore', 'Southern University of Science and Technology', 'National University of Singapore', 'The Chinese University of Hong Kong', 'NUS', 'The Chinese University of Hong Kong (CUHK)']"
Multi-Target Coverage with Connectivity Maintenance Using Knowledge-Incorporated Policy Framework,"This paper considers a multi-target coverage problem where a robot team aims to efficiently cover multi-targets while maintaining connectivity in a distributed manner. A novel knowledge-incorporated policy framework is proposed to derive a distributed, efficient, and connectivity guaranteed coverage policy. In particular, a knowledge-guided policy network (KGPnet) is designed, which consists of observation attention representation, interaction attention representation, and knowledge-guided policy learning. Giving credit to the KGPnet, the connectivity guaranteed coverage policy can be applied to different number targets. Moreover, based on the knowledge of the algebraic connectivity and coverage rate, a comprehensive reward is designed to guide the training of the behavior of multi-target coverage with connectivity maintenance. Furthermore, since the policy learned through deep reinforcement learning (DRL) can not guarantee the connectivity of the robot team, a knowledge-nested policy filtering is designed to filter dis-connectivity policies to satisfy the connectivity constraint based on the knowledge model of connectivity maintenance. Various simulations are conducted to verify the effectiveness of the proposed method. Besides, numerous real-world experiments with three-wheel omnidirectional cars and a motion capture system are presented to demonstrate the practicability of the proposed method.
                           ","['Wu, Shiguang', 'Pu, Zhiqiang', 'Liu, Zhen', 'Qiu, Tenghai', 'Yi, Jianqiang', 'Zhang, Tianle']","['Chinese Academy of Sciences Beijing, China', 'University of Chinese Academy of Sciences; Institute of Automati', 'Institude of Automation, Chinese Academy of Sciences', 'CASIA', 'Chinese Academy of Sciences', 'Institute of Automation, Chinese Academy of Sciences, Beijing']"
Proactive Action Visual Residual Reinforcement Learning for Contact-Rich Tasks Using a Torque-Controlled Robot,"Contact-rich manipulation tasks are commonly found in modern manufacturing settings. However, manually designing a robot controller is considered hard for traditional control methods as the controller requires an effective combination of modalities and vastly different characteristics. In this paper, we first consider incorporating operational space visual and haptic information into a reinforcement learning (RL) method to solve the target uncertainty problems in unstructured environments. Moreover, we propose a novel idea of introducing a proactive action to solve a partially observable Markov decision process (POMDP) problem. With these two ideas, our method can either adapt to reasonable variations in unstructured environments or improve the sample efficiency of policy learning. We evaluated our method on a task that involved inserting a random-access memory (RAM) using a torque-controlled robot and tested the success rates of different baselines used in the traditional methods. We proved that our method is robust and can tolerate environmental variations.
                           ","['Shi, Yunlei', 'Chen, Zhaopeng', 'Liu, Hongxu', 'Riedel, Sebastian Danilo', 'Gao, Chunhui', 'Feng, Qian', 'Deng, Jun', 'Zhang, Jianwei']","['Universit?t Hamburg', 'University of Hamburg', 'Technical University of Munich', 'German Aerospace Center (DLR), Robotics and Mechatronics Center', 'Agile Robots AG', 'Technical University of Munich', 'Agile Robots AG', 'University of Hamburg']"
Reinforcement Learning Control of a Novel Magnetic Actuated Flexible-Joint Robotic Camera System for Single Incision Laparoscopic Surgery,"This paper describes the control of a novel Magnetic Actuated Flexible-joint Robotic Surgical (MAFRS) camera system with four degrees of freedom for single incision laparoscopic surgery. Based on the idea of motion decoupling, we designed a novel MAFRS system which is consists of an external driving device and a motor-free insertable wireless robotic device with a hollow flexible joint. Due to the problems of abdominal wall obstruction and variability in abdominal wall thickness during the actual application of the MAFRS system, as well as the existence of multiple permanent magnets and magnetically conductive media, high-precision position and attitude control of the insertable device without onboard motors has always been a challenge. We use the external driving device to generate a magnetic field to control the position and attitude of the internal robotic device. Aiming at the automatic precise tilt motion control of the novel MAFRS system, we have developed a closed-loop control scheme using the Deep Deterministic Policy Gradient algorithm. By referring to the damping characteristics of human muscles, a virtual-muscle method is proposed to eliminate the chattering problem of the MAFRS camera at specific angles. The experimental investigations indicate that the internal robotic device can be effectively controlled under different abdominal wall thicknesses. The tilt motion control accuracy is within 0.5¡ã, and it has good adaptability and anti-interference performance.
                           ","['Xu, Dong', 'Zhang, Yuanlin', 'Tan, Wenshuai', 'Wei, Hongxing']","['Beihang University', 'Beihang University', 'Beihang University', 'Beihang University']"
Continuous Transition: Improving Sample Efficiency for Continuous Control Problems Via MixUp,"Although deep reinforcement learning (RL) has been successfully applied to a variety of robotic control tasks, it's still challenging to apply it to real-world tasks, due to the poor sample efficiency. Attempting to overcome this shortcoming, several works focus on reusing the collected trajectory data during the training by decomposing them into a set of policy-irrelevant discrete transitions. However, their improvements are somewhat marginal since i) the amount of the transitions is usually small, and ii) the value assignment only happens in the joint states. To address these issues, this paper introduces a concise yet powerful method to construct Continuous Transition, which exploits the trajectory information by exploiting the potential transitions along the trajectory. Specifically, we propose to synthesize new transitions for training by linearly interpolating the consecutive transitions. To keep the constructed transitions authentic, we also develop a discriminator to guide the construction process automatically. Extensive experiments demonstrate that our proposed method achieves a significant improvement in sample efficiency on various complex continuous robotic control problems in MuJoCo and outperforms the advanced model-based / model-free RL methods. The source code is available at https://github.com/junfanlin/continuous-transition
                           ","['Lin, Junfan', 'Huang, Zhongzhan', 'Wang, Keze', 'Liang, Xiaodan', 'Chen, Weiwei', 'Lin, Liang']","['Sun Yat-Sen University', 'Sun Yat-Sen University', 'University of California, Los Angeles', 'Sun Yat-Sen University', 'DMAI', 'Sun Yat-Sen University']"
DIMSAN: Fast Exploration with the Synergy between Density-Based Intrinsic Motivation and Self-Adaptive Action Noise,"Exploration in environments with sparse rewards remains a challenging research problem in Deep Reinforcement Learning (DRL). For the off-policy method, it usually needs a large number of training samples. With the growing dimensions of state and action space, this method becomes more and more sample-inef?cient. In this paper, we propose a novel fast exploration method for off-policy reinforcement learning, called Density-based Intrinsic Motivation and Self-adaptive Action Noise (DIMSAN). Our main contribution is twofold: (1) We propose a Density-based Intrinsic Motivation (DIM) method. It introduces a new intrinsic-reward generation mechanism based on samples¡¯ density estimation during experience replay and encourages the agent to seek novel and unfamiliar states. (2) We propose a Self-adaptive Action Noise (SAN) to deal with the exploration-exploitation tradeoffs, which could automatically change the exploration step through adding adaptive action space noise. The synergy between DIM and SAN could guide the agent to search the state and action space with high ef?ciency. We evaluate our method on the benchmark manipulation tasks and the designed challenging ones. Empirical results show that our method outperforms the existing methods in terms of convergence speed and sample ef?ciency, especially in challenging tasks.
                           ","['Li, Jiayi', 'Li, Boyao', 'Lu, Tao', 'Lu, Ning', 'Cai, Yinghao', 'Wang, Shuo']","['Institute of Automation, Chinese Academy of Sciences', 'China Academy of Launch Vehicle Technology', 'The Hi-Tech Innovation Engineering Center', 'Institute of Automation, Chinese Academy of Sciences', 'Institute of Automation, Chinese Academy of Sciences', 'Chinese Academy of Sciences']"
Reinforced iLQR: A Sample-Efficient Robot Locomotion Learning,"Robot locomotion is a major challenge in robotics. Model-based approaches are vulnerable to model errors, and incur high computation overhead resulted from long control horizon. Model-free approaches are trained with a large number of training samples, which are expensive to obtain. In this paper, we develop a hybrid control and learning framework, called Reinforced iLQR (RiLQR), which combines the advantages of model-based iLQR control with model-free RL policy learning to simultaneously achieve high sample efficiency, low computation overhead, and high robustness against model errors in robot locomotion. Through extensive evaluation on the Mujoco platform, we demonstrate that RiLQR outperforms the state-of-the-art model-based and model-free baselines by big margins in a set of tasks with different complexities.
                           ","['Zong, Tongyu', 'Sun, Liyang', 'Liu, Yong']","['New York University', 'New York University', 'New York University']"
Reinforcement Learning for Autonomous Driving with Latent State Inference and Spatial-Temporal Relationships,"Deep reinforcement learning (DRL) provides a promising way for learning navigation in complex autonomous driving scenarios. However, identifying the subtle cues that can indicate drastically different outcomes remains an open problem with designing autonomous systems that operate in human environments. In this work, we show that explicitly inferring the latent state and encoding spatial-temporal relationships in a reinforcement learning framework can help address this difficulty. We encode prior knowledge on the latent states of other drivers through a framework that combines the reinforcement learner with a supervised learner. In addition, we model the influence passing between different vehicles through graph neural networks (GNNs). The proposed framework significantly improves performance in the context of navigating T-intersections compared with state-of-the-art baseline approaches.
                           ","['Ma, Xiaobai', 'Li, Jiachen', 'Kochenderfer, Mykel', 'Isele, David', 'Fujimura, Kikuo']","['Stanford University', 'University of California, Berkeley', 'Stanford University', 'University of Pennsylvania, Honda Research Institute USA', 'Honda Research Institute']"
Learning Task-Oriented Dexterous Grasping from Human Knowledge,"Industrial automation requires robot dexterity to automate many processes such as product assembling, packaging, and material handling. The existing robotic systems lack the capability to determining proper grasp strategies in the context of object affordances and task designations. In this paper, a framework of task-oriented dexterous grasping is proposed to learn grasp knowledge from human experience and to deploy the grasp strategies while adapting to grasp contexts. Grasp topology is defined and grasp strategies are learned from an established dataset for task-oriented dexterous manipulation. To adapt to various grasp contexts, a reinforcement-learning based grasping policy was implemented to deploy different task-oriented strategies. The performance of the system was evaluated in a simulated grasping environment by using an AR10 anthropomorphic hand installed in a Sawyer robotic arm. The proposed framework achieved a hit rate of 100% for grasp strategies and an overall top-3 match rate of 95.6%. The success rate of grasping was 85.6% during 2700 grasping experiments for manipulation tasks given in natural-language instructions.
                           ","['Li, Hui', 'Zhang, Yinlong', 'Li, Yanan', 'He, Hongsheng']","['Wichita State University', 'Shenyang Institute of Automation, Chinese Academy of Sciences', 'University of Sussex', 'Wichita State University']"
Hierarchies of Planning and Reinforcement Learning for Robot Navigation,"Solving robotic navigation tasks via reinforcement learning (RL) is challenging due to their sparse reward and long decision horizon nature. However, in many navigation tasks, high-level (HL) task representations, like a rough floor plan, are available. Previous work has demonstrated efficient learning by hierarchal approaches consisting of path planning in the HL representation and using sub-goals derived from the plan to guide the RL policy in the source task. However, these approaches usually neglect the complex dynamics and sub-optimal sub-goal-reaching capabilities of the robot during planning. This work overcomes these limitations by proposing a novel hierarchical framework that utilizes a trainable planning policy for the HL representation. Thereby robot capabilities and environment conditions can be learned utilizing collected rollout data. We specifically introduce a planning policy based on value iteration with a learned transition model (VI-RL). In simulated robotic navigation tasks, VI-RL results in consistent strong improvement over vanilla RL, is on par with vanilla hierarchal RL on single layouts but more broadly applicable to multiple layouts, and is on par with trainable HL path planning baselines except for a parking task with difficult non-holonomic dynamics where it shows marked improvements.
                           ","['W?hlke, Jan', 'Schmitt, Felix', 'van Hoof, Herke']","['Bosch Center for Artificial Intelligence', 'Bosch Center for Artificial Intelligence', 'University of Amsterdam']"
Deep Reinforcement Learning for Mapless Navigation of a Hybrid Aerial Underwater Vehicle with Medium Transition,"Since the application of Deep Q-Learning to the continuous action domain in Atari-like games, Deep Reinforcement Learning (Deep-RL) techniques for motion control have been qualitatively enhanced. Nowadays, modern Deep-RL can be successfully applied to solve a wide range of complex decision-making tasks for many types of vehicles. Based on this context, in this paper, we propose the use of Deep-RL to perform autonomous mapless navigation for Hybrid Unmanned Aerial Underwater Vehicles (HUAUVs), robots that can operate in both, air or water media. We developed two approaches, one deterministic and the other stochastic. Our system uses the relative localization of the vehicle and simple sparse range data to train the network. We compared our approaches with an adapted version of the BUG2 algorithm for mapless navigation of aerial vehicles. Based on experimental results, we can conclude that Deep-RL-based approaches can be successfully used to perform mapless navigation and obstacle avoidance for HUAUVs. Our vehicle accomplished the navigation in two scenarios, being capable to achieve the desired target through both environments, and even outperforming the behavior-based algorithm on the obstacle-avoidance capability.
                           ","['Grando, Ricardo', 'Costa de Jesus, Junior', 'Kich, Victor Augusto', 'Kolling, Alisson Henrique', 'Bortoluzzi, Nicolas', 'Miranda Pinheiro, Pedro', 'Alves Neto, Armando', 'Drews-Jr, Paulo']","['Federal University of Rio Grande', 'Universidade Federal De Santa Maria', 'Universidade Federal De Santa Maria', 'Universidade Federal De Santa Maria', 'Federal University of Rio Grande, FURG', 'Federal University of Rio Grande - FURG', 'Universidade Federal De Minas Gerais', 'Federal University of Rio Grande (FURG)']"
Deep Reinforcement Learning for Active Target Tracking,"We solve active target tracking, one of the essential tasks in autonomous systems, using a deep reinforcement learning (RL) approach. In this problem, an autonomous agent is tasked with acquiring information about targets of interests using its on-board sensors. The classical challenges in this problem are system model dependence and the difficulty of computing information-theoretic cost functions for a long planning horizon. RL provides solutions for these challenges as the length of its effective planning horizon does not affect the computational complexity, and it drops the strong dependency of an algorithm on system models. In particular, we introduce Active Tracking Target Network (ATTN), a unified deep RL policy that is capable of solving major sub-tasks of active target tracking -- in-sight tracking, navigation, and exploration. The policy shows robust behavior for tracking agile and anomalous targets with a partially known target model. Additionally, the same policy is able to navigate in obstacle environments to reach distant targets as well as explore the environment when targets are positioned in unexpected locations.
                           ","['Jeong, Heejin', 'Hassani, Hamed', 'Morari, Manfred', 'Lee, Daniel', 'Pappas, George J.']","['University of Pennsylvania', 'University of Pennsylvania', 'ETH Zurich', 'Cornell Tech', 'University of Pennsylvania']"
Regularizing Action Policies for Smooth Control with Reinforcement Learning,"A critical problem with the practical utility of controllers trained with deep Reinforcement Learning (RL) is the notable lack of smoothness in the actions learned by the RL policies. This trend often presents itself in the form of control signal oscillation and can result in poor control, high power consumption, and undue system wear. We introduce Conditioning for Action Policy Smoothness (CAPS), an effective yet intuitive regularization on action policies, which offers consistent improvement in the smoothness of the learned state-to-action mappings of neural network controllers, reflected in the elimination of high-frequency components in the control signal. Tested on a real system, improvements in controller smoothness on a quadrotor drone resulted in an almost 80% reduction in power consumption while consistently training flight-worthy controllers. Project website: http://ai.bu.edu/caps
                           ","['Mysore, Siddharth', 'El Mabsout, Bassel', 'Mancuso, Renato', 'Saenko, Kate']","['Boston University', 'Boston University', 'Boston University', 'Boston University']"
Stabilizing Neural Control Using Self-Learned Almost Lyapunov Critics,"The lack of stability guarantee restricts the practical use of learning-based methods in core control problems in robotics. We develop new methods for learning neural control policies and neural Lyapunov critic functions in the model-free reinforcement learning (RL) setting. We use sample-based approaches and the Almost Lyapunov function conditions to estimate the region of attraction and invariance properties through the learned Lyapunov critic functions. The methods enhance stability of neural controllers for various nonlinear systems including automobile and quadrotor control.
                           ","['Chang, Ya-Chien', 'Gao, Sicun']","['University of California San Diego', 'UCSD']"
LBGP: Learning Based Goal Planning for Autonomous Following in Front,"This paper investigates a hybrid solution which combines deep reinforcement learning (RL) and classical trajectory planning for the ``following in front'' application. Here, an autonomous robot aims to stay ahead of a person as the person freely walks around. Following in front is a challenging problem as the user's intended trajectory is unknown and needs to be estimated, explicitly or implicitly, by the robot. In addition, the robot needs to find a feasible way to safely navigate ahead of human trajectory. Our deep RL module makes decisions at a high level by implicitly estimates the human trajectory and produces short-term navigational goals to guide the robot. These goals are used by a trajectory planner, which is responsible for low-level execution, to smoothly navigate the robot to the short-term goals, and eventually in front of the user. We employ curriculum learning in the deep RL module to efficiently achieve a high return. Our system outperforms the state-of-the-art in following ahead and is more reliable compared to end-to-end alternatives in both the simulation and real world experiments. In contrast to a pure deep RL approach, we demonstrate zero-shot transfer of the trained policy from simulation to the real world.
                           ","['Nikdel, Payam', 'Vaughan, Richard', 'Chen, Mo']","['Simon Fraser University', 'Simon Fraser University', 'Simon Fraser University']"
Evaluating Guided Policy Search for Human-Robot Handovers,"We evaluate the potential of Guided Policy Search (GPS), a model-based reinforcement learning (RL) method, to train a robot controller for human-robot object handovers. Handovers are a key competency for collaborative robots and GPS could be a promising approach for this task, as it is data efficient and does not require prior knowledge of the robot and environment dynamics. However, existing uses of GPS did not consider important aspects of human-robot handovers, namely large spatial variations in reach locations, moving targets, and generalizing over mass changes induced by the object being handed over. In this work, we formulate the reach phase of handovers as an RL problem and then train a collaborative robot arm in a simulation environment. Our results indicate that GPS is limited in the spatial generalizability over variations in the target location, but that this issue can be mitigated with the addition of local controllers trained over target locations in the high error regions. Moreover, learned policies generalize well over a large range of end-effector masses. Moving targets can be reached with comparable errors using a global policy trained on static targets, but this results in inefficient, high-torque, trajectories. Training on moving targets improves trajectories, but results in worse worst-case performance. Initial results suggest that lower-dimensional state representations are beneficial for GPS performance in handovers.
                           ","['Kshirsagar, Alap', 'Hoffman, Guy', 'Biess, Armin']","['Cornell University', 'Cornell University', 'Ben-Gurion University of the Negev']"
Decentralized Structural-RNN for Robot Crowd Navigation with Deep Reinforcement Learning,"Safe and efficient navigation through human crowds is an essential capability for mobile robots. Previous work on robot crowd navigation assumes that the dynamics of all agents are known and well-defined. In addition, the performance of previous methods deteriorates in partially observable environments and environments with dense crowds. To tackle these problems, we propose decentralized structural-Recurrent Neural Network (DS-RNN), a novel network that reasons about spatial and temporal relationships for robot decision making in crowd navigation. We train our network with model-free deep reinforcement learning without any expert supervision. We demonstrate that our model outperforms previous methods in challenging crowd navigation scenarios. We successfully transfer the policy learned in the simulator to a real-world TurtleBot 2i.
                           ","['Liu, Shuijing', 'Chang, Peixin', 'Liang, Weihang', 'Chakraborty, Neeloy', 'Driggs-Campbell, Katherine']","['University of Illinois at Urbana Champaign', 'University of Illinois at Urbana Champaign', 'University of Illinois at Urbana-Champaign', 'University of Illinois at Urbana-Champaign', 'University of Illinois at Urbana-Champaign']"
Causal Reasoning in Simulation for Structure and Transfer Learning of Robot Manipulation Policies,"We present CREST, an approach for causal reasoning in simulation to learn the relevant state space for a robot manipulation policy. Our approach conducts interventions using internal models, which are simulations with approximate dynamics and simplified assumptions. These interventions elicit the structure between the state and action spaces, enabling construction of neural network policies with only relevant states as input. These policies are pretrained using the internal model with domain randomization over the relevant states. The policy network weights are then transferred to the target domain (e.g., the real world) for fine tuning. We perform extensive policy transfer experiments in simulation for two representative manipulation tasks: block stacking and crate opening. Our policies are shown to be more robust to domain shifts, more sample efficient to learn, and scale to more complex settings with larger state spaces. We also show improved zero-shot sim-to-real transfer of our policies for the block stacking task.
                           ","['Lee, Timothy Edward', 'Zhao, Jialiang', 'Sawhney, Amrita', 'Girdhar, Siddharth', 'Kroemer, Oliver']","['Carnegie Mellon University', 'Carnegie Mellon University', 'Carnegie Mellon University', 'Carnegie Mellon University', 'Carnegie Mellon University']"
Amortized Q-Learning with Model-Based Action Proposals for Autonomous Driving on Highways,"Well-established optimization-based methods can guarantee an optimal trajectory for a short optimization horizon, typically no longer than a few seconds. As a result, choosing the optimal trajectory for this short horizon may still result in a sub-optimal long-term solution. At the same time, the resulting short-term trajectories allow for effective, comfortable and provable safe maneuvers in a dynamic traffic environment.In this work, we address the question of how to ensure an optimal long-term driving strategy, while keeping the benefits of classical trajectory planning. We introduce a Reinforcement Learning based approach that coupled with a trajectory planner, learns an optimal long-term decision-making strategy for driving on highways.By online generating locally optimal maneuvers as actions, we balance between the infinite low-level continuous action space, and the limited flexibility of a fixed number of predefined standard lane-change actions. We evaluated our method on realistic scenarios in the open-source traffic simulator SUMO and were able to achieve better performance than the 4 benchmark approaches we compared against, including a random action selecting agent, greedy agent, high-level, discrete actions agent and an IDM-based SUMO-controlled agent.
                           ","['Mirchevska, Branka', 'Huegle, Maria', 'Kalweit, Gabriel', 'Werling, Moritz', 'Boedecker, Joschka']","['Freiburg University, BMW Group', 'University of Freiburg', 'University of Freiburg', 'Karlsruhe Institute of Technology', 'University of Freiburg']"
Towards Efficient Multiview Object Detection with Adaptive Action Prediction,"Active vision is a desirable perceptual feature for robots. Existing approaches usually make strong assumptions about the task and environment, thus are less robust and efficient. This study proposes an adaptive view planning approach to boost the efficiency and robustness of active object detection. We formulate the multi-object detection task as an active multiview object detection problem given the initial location of the objects. Next, we propose a novel adaptive action prediction (A2P) method built on a deep Q-learning network with a dueling architecture. The A2P method is able to perform view planning based on visual information of multiple objects; and adjust action ranges according to the task status. Evaluated on the AVD dataset, A2P leads to 21.9% increase in detection accuracy in unfamiliar environments, while improving efficiency by 22.7%. On the T-LESS dataset, multi-object detection boosts efficiency by more than 30% while achieving equivalent detection accuracy.
                           ","['Xu, Qianli', 'Fang, Fen', 'Gauthier, Nicolas', 'Liang, Wenyu', 'Wu, Yan', 'Li, Liyuan', 'Lim, Joo Hwee']","['Institute for Infocomm Research', 'I2R', 'Institute for Infocomm Research, Agency for Science, Technology', 'Institute for Infocomm Research, A*STAR', 'A*STAR Institute for Infocomm Research', 'Institute for Infocomm Research, A*STAR', 'I2R A*STAR']"
Reward Learning from Very Few Demonstrations (I),"
Keywords: Learning from Demonstration, Reinforcement Learning, Visual Learning
","['Eteke, Cem', 'Kebude, Dogancan', 'Akgun, Baris']","['Koc University', 'Koc University', 'Koc University']"
Harmonic-Based Optimal Motion Planning in Constrained Workspaces Using Reinforcement Learning,"In this work, we propose a novel reinforcement learning algorithm to solve the optimal motion planning problem. Particular emphasis is given on the rigorous mathematical proof of safety, convergence as well as optimality w.r.t. to a classical integral quadratic cost function, while reinforcement learning is adopted to enable its approximation. Both offline and online solutions are presented, and an implementation of the offline method is contrasted to a state-of-the-art RRT^{star} approach. This novel approach inherits the strong traits from both artificial potential fields, i.e., reactivity, as well as sampling-based methods, i.e., optimality, and opens up new paths to the age-old problem of motion planning, by merging modern tools and philosophies from various corners of the field.
                           ","['Rousseas, Panagiotis', 'Bechlioulis, Charalampos', 'Kyriakopoulos, Kostas']","['NTUA', 'National Technical University of Athens', 'National Technical Univ. of Athens']"
Tiny Robot Learning (tinyRL) for Source Seeking on a Nano Quadcopter,"We present fully autonomous source seeking onboard a highly constrained nano quadcopter, by contributing application-specific system and observation feature design to enable inference of a deep-RL policy onboard a nano quadcopter. Our deep-RL algorithm finds a high-performance solution to a challenging problem, even in presence of high noise levels and generalizes across real and simulation environments with different obstacle configurations. We verify our approach with simulation and in-field testing on a CrazyFlie using only the cheap and ubiquitous Cortex-M4 microcontroller unit. The results show that by end-to-end application-specific system design, our contribution consumes almost three times less additional power, as compared to a competitive learning-based navigation approach onboard a nano quadcopter. Thanks to our observation space, which we carefully design within the resource constraints, our solution achieves a 94% success rate in cluttered and randomized test environments, as compared to the previously achieved 80%. We also compare our strategy to a simple finite state machine (FSM), geared towards efficient exploration, and demonstrate that our policy is more robust and resilient at obstacle avoidance as well as up to 70% more efficient in source seeking. To this end, we contribute a cheap and lightweight end-to-end tiny robot learning (tinyRL) solution, running onboard a nano quadcopter, that proves to be robust and efficient in a challenging task.
                           ","['Duisterhof, Bardienus Pieter', 'Krishnan, Srivatsan', 'Cruz, Jonathan Jesus', 'Banbury, Colby Richard', 'Fu, William', 'Faust, Aleksandra', 'de Croon, Guido', 'Janapa Reddi, Vijay']","['Harvard University', 'Harvard University', 'Harvard University', 'Harvard University', 'Harvard College', 'Google Brain', 'TU Delft / ESA', 'Harvard University']"
Residual Model Learning for Microrobot Control,"A majority of microrobots are constructed using compliant materials that are difficult to model analytically, limiting the utility of traditional model-based controllers. Challenges in data collection on micro-robots and large errors between simulated models and real robots make current model-based learning and sim-to-real transfer methods difficult to apply. We propose a novel framework residual model learning (RML) that leverages approximate models to substantially reduce the sample complexity associated with learning an accurate robot model. We show that using RML, we can learn a model of the Harvard Ambulatory MicroRobot (HAMR) using just 12 seconds of passively collected interaction data. The learned model is accurate enough to be leveraged as ""proxy-simulator"" for learning walking and turning behaviors using model-free reinforcement learning algorithms. RML provides a general framework for learning from extremely small amounts of interaction data, and our experiments with HAMR clearly demonstrate that RML substantially outperforms existing techniques.
                           ","['Gruenstein, Joshua', 'Chen, Tao', 'Doshi, Neel', 'Agrawal, Pulkit']","['Massachusetts Institute of Technology', 'Massachusetts Institute of Technology', 'MIT', 'MIT']"
Data-Based Control of Partially-Observed Robotic Systems,"This paper presents a data-based approach to control robotic systems with partially-observed feedback. First, an open-loop optimization problem is solved to generate the nominal trajectory and then a linear time-varying Autoregressive¨CMoving-Average (ARMA) model of the system is calculated along the trajectory from the output measurement data. The system is then described in an information state containing input-output information of the past few steps following a feedback gain which is calculated by solving a specific LQG problem along the nominal trajectory. The separate design of the open-loop and the closed-loop problem is used following the Decoupled Data-based Control (D2C) approach. Simulation results are also shown for complex models with fluid-structure interaction in the presence of both process and measurement noise.
                           ","['Wang, Ran', 'Goyal, Raman', 'Chakravorty, Suman', 'Skelton, Robert']","['Texas A&M University', 'Texas A&M University', 'Texas A&M University', 'Texas A&M University']"
Generalization in Reinforcement Learning by Soft Data Augmentation,"Extensive efforts have been made to improve the generalization ability of Reinforcement Learning (RL) methods via domain randomization and data augmentation. However, as more factors of variation are introduced during training, optimization becomes increasingly challenging, and empirically may result in lower sample efficiency and unstable training. Instead of learning policies directly from augmented data, we propose SOft Data Augmentation (SODA), a method that decouples augmentation from policy learning. Specifically, SODA imposes a soft constraint on the encoder that aims to maximize the mutual information between latent representations of augmented and non-augmented data, while the RL optimization process uses strictly non-augmented data. Empirical evaluations are performed on diverse tasks from DeepMind Control suite as well as a robotic manipulation task, and we find SODA to significantly advance sample efficiency, generalization, and stability in training over state-of-the-art vision-based RL methods.
                           ","['Hansen, Nicklas', 'Wang, Xiaolong']","['Technical University of Denmark', 'UC San Diego']"
Efficient Robotic Object Search Via HIEM: Hierarchical Policy Learning with Intrinsic-Extrinsic Modeling,"Despite the significant success at enabling robots with autonomous behaviors makes deep reinforcement learning a promising approach for robotic object search task, the deep reinforcement learning approach severely suffers from the nature sparse reward setting of the task. To tackle this challenge, we present a novel policy learning paradigm for the object search task, based on hierarchical and interpretable modeling with an intrinsic-extrinsic reward setting. More specifically, we explore the environment efficiently through a proxy low-level policy which is driven by the intrinsic rewarding sub-goals. We further learn our hierarchical policy from the efficient exploration experience where we optimize both of our high-level and low-level policies towards the extrinsic rewarding goal to perform the object search task well. Experiments conducted on the House3D environment validate and show that the robot, trained with our model, can perform the object search task in a more optimal and interpretable way.
                           ","['Ye, Xin', 'Yang, Yezhou']","['Arizona State University', 'Arizona State University']"
Nonholonomic Yaw Control of an Underactuated Flying Robot with Model-Based Reinforcement Learning,"Nonholonomic control is a candidate to control nonlinear systems with path-dependant states. We investigate an underactuated flying micro-aerial-vehicle, the ionocraft, that requires nonholonomic control in the yaw-direction for complete attitude control. Deploying an analytical control law involves substantial engineering design and is sensitive to inaccuracy in the system model. With specific assumptions on assembly and system dynamics, we derive a Lie bracket for yaw control of the ionocraft. As a comparison to the significant engineering effort required for an analytic control law, we implement a data-driven model-based reinforcement learning yaw controller in a simulated flight task. We demonstrate that a simple model- based reinforcement learning framework can match the derived Lie bracket control ¨C in yaw rate and chosen actions ¨C in a few minutes of flight data, without a pre-defined dynamics function. This paper shows that learning-based approaches are useful as a tool for synthesis of nonlinear control laws previously only addressable through expert-based design.
                           ","['Lambert, Nathan', 'Schindler, Craig', 'Drew, Daniel S.', 'Pister, Kristofer S. J.']","['University of California, Berkeley', 'University of California, Berkeley', 'Stanford University', 'University of California, Berkeley']"
Learning to Herd Agents Amongst Obstacles: Training Robust Shepherding Behaviors Using Deep Reinforcement Learning,"Robotic shepherding problem considers the control and navigation of a group of coherent agents (e.g., a flock of bird or a fleet of drones) through the motion of an external robot, called shepherd. Machine learning based methods have successfully solved this problem in an environment with no obstacles. Rule-based methods, on the other hand, can handle more complex scenarios in which environments are cluttered with obstacles and allow multiple shepherds to work collaboratively. However, these rule-based methods are fragile due to the difficulty in defining a comprehensive set of behavioral rules. To overcome these limitations, we propose the first known learning-based method that can herd agents amongst obstacles. By using deep reinforcement learning techniques combined with the probabilistic roadmaps, we train a shepherding model using noisy but controlled environmental and behavioral parameters. Our experimental results show that the trained shepherding controller is robust, namely, it is insensitive to the uncertainties originated from either the group behavioral models or from environments with a small of path homotopy classes. Consequently, the proposed method has a higher success rate, shorter completion time and path length than the rule-based behavioral methods have. These advantages are particularly prominent in more challenging scenarios involving more difficult groups and strenuous passages.
                           ","['Zhi, Jixuan', 'Lien, Jyh-Ming']","['George Mason University', 'George Mason University']"
Double Meta-Learning for Data Efficient Policy Optimization in Non-Stationary Environments,"We are interested in learning models of non-stationary environments, which can be framed as a multi-task learning problem. Model-free reinforcement learning algorithms can achieve good asymptotic performance in multi-task learning at a cost of extensive sampling, due to their approach, which requires learning from scratch. While model-based approaches are among the most data efficient learning algorithms, they still struggle with complex tasks and model uncertainties. Meta-reinforcement learning addresses the efficiency and generalization challenges on multi task learning by quickly leveraging the meta-prior policy for a new task. In this paper, we propose a meta-reinforcement learning approach to learn the dynamic model of a non-stationary environment to be used for meta-policy optimization later. Due to the sample efficiency of model-based learning methods, we are able to simultaneously train both the meta-model of the non-stationary environment and the meta-policy until dynamic model convergence. Then, the meta-learned dynamic model of the environment will generate simulated data for meta-policy optimization. Our experiment demonstrates that our proposed method can meta-learn the policy in a non-stationary environment with the data efficiency of model-based learning approaches while achieving the high asymptotic performance of model-free meta-reinforcement learning.
                           ","['Aghapour, Elahe', 'Ayanian, Nora']","['University of Southern California', 'University of Southern California']"
Leveraging Post Hoc Context for Faster Learning in Bandit Settings with Applications in Robot-Assisted Feeding,"Autonomous robot-assisted feeding requires the ability to acquire a wide variety of food items. However, it is impossible for such a system to be trained on all types of food in existence. Therefore, a key challenge is choosing a manipulation strategy for a previously unseen food item. Previous work showed that the problem can be represented as a linear bandit with visual context. However, food has a wide variety of multi-modal properties relevant to manipulation that can be hard to distinguish visually. Our key insight is that we can leverage the haptic context we collect during and after manipulation (i.e., ``post hoc'') to learn some of these properties and more quickly adapt our visual model to previously unseen food. In general, we propose a modified linear contextual bandit framework augmented with post hoc context observed after action selection to empirically increase learning speed and reduce cumulative regret. Experiments on synthetic data demonstrate that this effect is more pronounced when the dimensionality of the context is large relative to the post hoc context or when the post hoc context model is particularly easy to learn. Finally, we apply this framework to the bite acquisition problem and demonstrate the acquisition of 8 previously unseen types of food with 21% fewer failures across 64 attempts.
                           ","['Gordon, Ethan Kroll', 'Roychowdhury, Sumegh', 'Bhattacharjee, Tapomayukh', 'Jamieson, Kevin', 'Srinivasa, Siddhartha']","['University of Washington', 'Indian Institute of Technology Kharagpur', 'University of Washington', 'University of Washington', 'University of Washington']"
Elastica: A Compliant Mechanics Environment for Soft Robotic Control,"Soft robots are notoriously hard to control. This is partly due to the scarcity of models and simulators able to capture their complex continuum mechanics, resulting in a lack of control methodologies that take full advantage of body compliance. Currently available methods are either too computational demanding or overly simplistic in their physical assumptions, leading to a paucity of available simulation resources for developing such control schemes. To address this, we introduce Elastica, an open-source simulation environment modeling the dynamics of soft, slender rods that can bend, twist, shear, and stretch. We couple Elastica with five state-of-the-art reinforcement learning (RL) algorithms (TRPO, PPO, DDPG, TD3, and SAC). We successfully demonstrate distributed, dynamic control of a soft robotic arm in four scenarios with both large action spaces, where RL learning is difficult, and small action spaces, where the RL actor must learn to interact with its environment. Training converges in 10 million policy evaluations with near real-time evaluation of learned policies.
                           ","['Naughton, Noel', 'Sun, Jiarui', 'Tekinalp, Arman', 'Parthasarathy, Tejaswin', 'Chowdhary, Girish', 'Gazzola, Mattia']","['University of Illinois at Urbana-Champaign', 'University of Illinois at Urbana, Champaign', 'University of Illinois at Urbana-Champaign', 'University of Illinois at Urbana-Champaign', 'University of Illinois at Urbana Champaign', 'University of Illinois at Urbana-Champaign']"
MaAST: Map Attention with Semantic Transformers for Efficient Visual Navigation,"Visual navigation for autonomous agents is a core task in the fields of computer vision and robotics. Learning-based methods, such as deep reinforcement learning, have the potential to outperform the classical solutions developed for this task; however, they come at a significantly increased computational load. Through this work, we design a novel approach that focuses on performing better or comparable to the existing learning-based solutions but under a clear time/computational budget. To this end, we propose a method to encode vital scene semantics such as traversable paths, unexplored areas, and observed scene objects--alongside raw visual streams such as RGB, depth, and semantic segmentation masks---into a semantically informed, top-down egocentric map representation. Further, to enable the effective use of this information, we introduce a novel 2-D map attention mechanism, based on the successful multi-layer Transformer networks. We conduct experiments on 3-D reconstructed indoor PointGoal visual navigation and demonstrate the effectiveness of our approach. We show that by using our novel attention schema and auxiliary rewards to better utilize scene semantics, we outperform multiple baselines trained with only raw inputs or implicit semantic information while operating with an 80% decrease in the agent's experience.
                           ","['Seymour, Zachary', 'Thopalli, Kowshik', 'Mithun, Niluthpol Chowdhury', 'Chiu, Han-Pang', 'Samarasekera, Supun', 'Kumar, Rakesh']","['SRI International', 'Arizona State University', 'SRI International', 'SRI International', 'SRI Sarnoff', 'SRI']"
Transfer Reinforcement Learning across Homotopy Classes,"The ability for robots to transfer their learned knowledge to new tasks¡ªwhere data is scarce¡ªis a fundamental challenge for successful robot learning. While fine-tuning has been well-studied as a simple but effective transfer approach in the context of supervised learning, it is not as well-explored in the context of reinforcement learning. In this work, we study the problem of fine-tuning in transfer reinforcement learning when tasks are parameterized by their reward functions, which are known beforehand. We conjecture that fine-tuning drastically underperforms when source and target trajectories are part of different homotopy classes. We demonstrate that fine-tuning policy parameters across homotopy classes	compared to fine-tuning within a homotopy class requires more interaction with the environment, and in certain cases is impossible. We propose a novel	fine-tuning algorithm, Ease-In-Ease-Out fine-tuning, that consists of a relaxing stage and a curriculum learning stage to enable transfer learning across homotopy classes. Finally, we evaluate our approach on several robotics-inspired simulated environments and empirically verify that the Ease-In-Ease-Out fine-tuning method can successfully fine-tune in a sample-efficient way compared to existing baselines.
                           ","['Cao, Zhangjie', 'Kwon, Minae', 'Sadigh, Dorsa']","['Stanford University', 'Stanford University', 'Stanford University']"
Coding for Distributed Multi-Agent Reinforcement Learning,"This paper aims to mitigate straggler effects in synchronous distributed learning for multi-agent reinforcement learning (MARL) problems. Stragglers arise frequently in a distributed learning system, due to the existence of various system disturbances such as slow-downs or failures of compute nodes and communication bottlenecks. To resolve this issue, we propose a coded distributed learning framework, which speeds up the training of MARL algorithms in the presence of stragglers, while maintaining the same accuracy as the centralized approach. As an illustration, a coded distributed version of the multi-agent deep deterministic policy gradient (MADDPG) algorithm is developed and evaluated. Different coding schemes, including maximum distance separable (MDS) code, random sparse code, replication-based code, and regular low density parity check (LDPC) code are also investigated. Simulations in several multi-robot problems demonstrate the promising performance of the proposed framework.
                           ","['Wang, Baoqian', 'Xie, Junfei', 'Atanasov, Nikolay']","['University of California, San Diego & San Diego State Universit', 'San Diego State University', 'University of California, San Diego']"
FISAR: Forward Invariant Safe Reinforcement Learning with a Deep Neural Network-Based Optimizer,"This paper investigates reinforcement learning with constraints, which are indispensable in safety-critical environments. To drive the constraint violation monotonically decrease, we take the constraints as Lyapunov functions and impose new linear constraints on the policy parameters' updating dynamics. As a result, the original safety set can be forward-invariant. However, because the new guaranteed-feasible constraints are imposed on the updating dynamics instead of the original policy parameters, classic optimization algorithms are no longer applicable. To address this, we propose to learn a generic deep neural network (DNN)-based optimizer to optimize the objective while satisfying the linear constraints. The constraint-satisfaction is achieved via projection onto a polytope formulated by multiple linear inequality constraints, which can be solved analytically with our newly designed metric. To the best of our knowledge, this is the textit{first} DNN-based optimizer for constrained optimization with the forward invariance guarantee. We show that our optimizer trains a policy to decrease the constraint violation and maximize the cumulative reward monotonically. Results on numerical constrained optimization and obstacle-avoidance navigation validate the theoretical findings.
                           ","['Sun, Chuangchuang', 'Kim, Dong Ki', 'How, Jonathan Patrick']","['Massachusetts Institute of Technology', 'Massachusetts Institute of Tech', 'Massachusetts Institute of Technology']"
Min-Max Entropy Inverse RL of Multiple Tasks,"Multi-task IRL recognizes that expert(s) could be switching between multiple ways of solving the same problem, or interleaving demonstrations of multiple tasks. The learner aims to learn the reward functions that individually guide these distinct ways. We present a new method for multi-task IRL that generalizes the well-known maximum entropy approach by combining it with a Dirichlet process based minimum entropy clustering of the observed data. This yields a single nonlinear optimization problem, called MinMaxEnt Multi-task IRL (MME-MTIRL), which can be solved using the Lagrangian relaxation and gradient descent methods. We evaluate MME-MTIRL on the robotic task of sorting onions on a processing line where the expert utilizes multiple ways of detecting and removing blemished onions. The method is able to learn the underlying reward functions to a high level of accuracy and it improves on the previous approaches.
                           ","['Arora, Saurabh', 'Doshi, Prashant', 'Banerjee, Bikramjit']","['University of Georgia', 'University of Georgia', 'University of Southern Mississippi']"
The Value of Planning for Infinite-Horizon Model Predictive Control,"Model Predictive Control (MPC) is a classic tool for optimal control of complex, real-world systems. Although it has been successfully applied to a wide range of challenging tasks in robotics, it is fundamentally limited by the prediction horizon, which, if too short, will result in myopic decisions. Recently, several papers have suggested using a learned value function as the terminal cost for MPC. If the value function is accurate, it effectively allows MPC to reason over an *infinite* horizon. Unfortunately, Reinforcement Learning (RL) solutions to value function approximation can be difficult to realize for robotics tasks. In this paper, we suggest a more efficient method for value function approximation that applies to goal-directed problems, like reaching and navigation. In these problems, MPC is often formulated to track a path or trajectory returned by a planner. However, this strategy is brittle in that unexpected perturbations to the robot will require replanning, which can be costly at runtime. Instead, we show how the intermediate data structures used by modern planners can be interpreted as an approximate *value function*. We show that that this value function can be used by MPC *directly*, resulting in more efficient and resilient behavior at runtime.
                           ","['Hatch, Nathan', 'Boots, Byron']","['University of Washington', 'University of Washington']"
MIDAS: Multi-Agent Interaction-Aware Decision-Making with Adaptive Strategies for Urban Autonomous Navigation,"Autonomous navigation in crowded, complex urban environments requires interacting with other agents on the road. A common solution to this problem is to use a prediction model to guess the likely future actions of other agents. While this is reasonable, it leads to overly conservative plans because it does not explicitly model the mutual influence of the actions of interacting agents. This paper builds a reinforcement learning-based method named MIDAS where an Ego agent learns to affect the control actions of other cars in urban driving scenarios. MIDAS uses an attention mechanism to handle an arbitrary number of other agents and includes a ''driver-type'' parameter to learn a single policy that works across different planning objectives. We build a simulation environment that enables diverse interaction experiments with a large number of agents and develop methods for quantitatively studying the safety, efficiency, and interaction among vehicles. MIDAS is validated using extensive experiments and we show that it (i) can work across different road geometries, (ii) results in an adaptive Ego policy that can be tuned easily to satisfy different performance criteria, such as aggressive or cautious driving, (iii) is robust to changes in the driving policies of external agents, and (iv) is safer and more efficient than existing approaches to interaction-aware decision-making.
                           ","['Chen, Xiaoyi', 'Chaudhari, Pratik']","['University of Pennsylvania', 'University of Pennsylvania']"
Learning Robust Driving Policies without Online Exploration,"We propose a multi-time-scale predictive representation learning method to efficiently learn robust driving policies in an offline manner that generalize well to novel road geometries, and damaged and distracting lane conditions which are not covered in the offline training data. We show that our proposed representation learning method can be applied easily in an offline (batch) reinforcement learning setting demonstrating the ability to generalize well and efficiently under novel conditions compared to standard batch RL methods. Our proposed method utilizes training data collected entirely offline in the real-world which removes the need of intensive online explorations that impede applying deep reinforcement learning on real-world robot training. Various experiments were conducted in both simulator and real-world scenarios for the purpose of evaluation and analysis of our proposed claims.
                           ","['Graves, Daniel', 'Nguyen, Nhat', 'Hassanzadeh, Kimia', 'Jin, Jun', 'Luo, Jun']","['Huawei Technologies Canada, Ltd', 'Huawei Technologies Canada', 'Huawei', 'University of Alberta', 'Huawei Technologies Canada']"
Shaped Policy Search for Evolutionary Strategies Using Waypoints,"In this paper, we try to better exploration in Blackbox methods, particularly Evolution strategies (ES), when applied to Reinforcement Learning (RL) problems where intermediate waypoints/subgoals are available. Since Evolutionary strategies are highly parallelizable, instead of extracting just a scalar cumulative reward, we use the state-action pairs from the trajectories obtained during rollouts/evaluations, to learn the dynamics of the agent. The learnt dynamics are then used in the optimization procedure to speed-up training. Lastly, we show how our proposed approach is universally applicable by presenting results from experiments conducted on Carla driving and UR5 robotic arm simulators.
                           ","['Lekkala, Kiran', 'Itti, Laurent']","['University of Southern California', 'University of Southern California']"
A Data-Driven Reinforcement Learning Solution Framework for Optimal and Adaptive Personalization of a Hip Exoskeleton,"Robotic exoskeletons are exciting technologies for augmenting human mobility. However, designing such a device for seamless integration with the human user and to assist human movement still is a major challenge. This paper aims at developing a novel data-driven solution framework based on reinforcement learning (RL), without first modeling the human-robot dynamics, to provide optimal and adaptive personalized torque assistance for reducing human efforts during walking. Our automatic personalization solution framework includes the assistive torque profile with two control timing parameters (peak and offset timings), the least square policy iteration (LSPI) for learning the parameter tuning policy, and a cost function based on a transferred work ratio. The proposed controller was successfully validated on a healthy human subject to assist unilateral hip extension in walking. The results showed that the optimal and adaptive RL controller as a new approach was feasible for tuning assistive torque profile of the hip exoskeleton that coordinated with human actions and reduced activation level of hip extensor muscle in human.
                           ","['Tu, Xikai', 'Li, Minhan', 'Liu, Ming', 'Si, Jennie', 'Huang, He (Helen)']","['North Carolina State University', 'North Carolina State University', 'North Carolina State University', 'Arizona State University', 'North Carolina State University and University of North Carolina']"
Natural Walking with Musculoskeletal Models Using Deep Reinforcement Learning,"Human gait optimality has been investigated recently, with the development of detailed musculoskeletal models, through trajectory optimization approaches or deep reinforcement learning (DRL). Trajectory optimization studies are limited by the trajectory length and can only generate open-loop solutions. While existing DRL solutions provide closed-loop control policies without trajectory length limit, they either do not evaluate the naturalness of the behaviour, or directly impose experimental tracking data. In this paper, a DRL-based approach is proposed with a nature-inspired curriculum learning (CL) scheme and a neuromechanically-inspired reward function. This approach generates close-to-natural human walking without the aid of experimental data. Our CL scheme is realized by an evolving reward function, targeting simpler behaviours such as standing and stepping first, then gradually refining the gait. The emerged gait from the closed-loop stochastic policy demonstrated a strong correlation with human gait kinematics, with Pearson correlations of 0.95 and 0.83 at the hip and knee, respectively, and higher gait symmetry than two other DRL-based control policies without CL. Our approach was also found to have efficient convergence to walking-capable policy. This approach can facilitate the development of assistive robotic systems by providing a ¡®human¡¯ controller, and could enable decentralized adaptation between the agent and the assistive robotic devices.
                           ","['Weng, Jiacheng', 'Hashemi, Ehsan', 'Arami, Arash']","['University of Waterloo', 'University of Alberta', 'University of Waterloo']"
Robust Policy Search for Robot Navigation,"Complex robot navigation and control problems can be framed as policy search problems. However, interactive learning in uncertain environments can be expensive, requiring the use of data-efficient methods. Bayesian optimization (BO) is an efficient nonlinear optimization method where queries are carefully selected to gather information about the optimum location. This is achieved by a surrogate model, which encodes past information, and the acquisition function for query selection. BO can be very sensitive to uncertainty in the input data or prior assumptions. In this work, we incorporate both robust optimization and statistical robustness, showing that both types of robustness are synergistic. For robust optimization we use an improved version of unscented BO which provides safe and repeatable policies in the presence of policy uncertainty. We also provide new theoretical insights. For statistical robustness, we use an adaptive surrogate model and we introduce the Boltzmann selection as a stochastic acquisition method to have convergence guarantees and improved performance even with surrogate modeling errors. We present results in several optimization benchmarks and robot tasks.
                           ","['Garcia-Barcos, Javier', 'Martinez-Cantin, Ruben']","['Universidad De Zaragoza', 'University of Zaragoza']"
Reinforcement Learning for Orientation Estimation Using Inertial Sensors with Performance Guarantee,"This paper presents a deep reinforcement learning (DRL) algorithm for orientation estimation using inertial sensors combined with a magnetometer. Lyapunov's method in control theory is employed to prove the convergence of orientation estimation errors. The estimator gains and a Lyapunov function are parametrised by deep neural networks and learned from samples based on the theoretical results. The DRL estimator is compared with three well-known orientation estimation methods on both numerical simulations and real dataset collected from commercially available sensors. The results show that the proposed algorithm is superior for arbitrary estimation initialisation and can adapt to a drastic angular velocity profile for which other algorithms can be hardly applicable. To the best of our knowledge, this is the first DRL-based orientation estimation method with an estimation error boundedness guarantee.
                           ","['Hu, Liang', 'Tang, Yujie', 'Zhou, Zhipeng', 'Pan, Wei']","['University of Essex', 'Delft University of Technology', 'Department of Cognitive Robotics, Delft University of Tec', 'Delft University of Technology']"
General-Sum Multi-Agent Continuous Inverse Optimal Control,"Modelling possible future outcomes of robot-human interactions is of importance in the intelligent vehicle and mobile robotics domains. Knowing the reward function that explains the observed behaviour of a human agent is advantageous for modelling the behaviour with Markov Decision Processes (MDPs). However, learning the rewards that determine the observed actions from data is complicated by interactions. We present a novel inverse reinforcement learning (IRL) algorithm that can infer the reward function in multi-agent interactive scenarios. In particular, the agents may act boundedly rational (i.e., sub-optimal), a characteristic that is typical for human decision making. Additionally, every agent optimizes its own reward function which makes it possible to address non-cooperative setups. In contrast to other methods, the algorithm does not rely on reinforcement learning during inference of the parameters of the reward function. We demonstrate that our proposed method accurately infers the ground truth reward function in two-agent interactive experiments.
                           ","['Neumeyer, Christian', 'Oliehoek, Frans', 'Gavrila, Dariu']","['TU Delft', 'Delft University of Technology', 'Delft University of Technology']"
Using Reinforcement Learning to Create Control Barrier Functions for Explicit Risk Mitigation in Adversarial Environments,"Air Combat is a high-risk activity carried out by trained professionals operating sophisticated equipment. During this activity, a number of trade-offs have to be made, such as the balance between risk and efficiency. A policy that minimizes risk could have very low efficiency, and one that maximizes efficiency may involve very high risk.In this study, we use Reinforcement Learning (RL) to create Control Barrier Functions (CBF) that captures the current risk, in terms of worst-case future separation between the aircraft and an enemy missile. CBFs are usually designed manually as closed-form expressions, but for a complex system such as a guided missile, this is not possible. Instead, we solve an RL problem using high fidelity simulation models to find value functions with CBF properties, that can then be used to guarantee safety in real air combat situations. We also provide a theoretical analysis of what family of RL problems result in value functions that can be used as CBFs in this way.The proposed approach allows the pilot in an air combat scenario to set the exposure level deemed acceptable and continuously monitor the risk related to his/her own safety. Given input regarding acceptable risk, the system limits the choices of the pilot to those that guarantee future satisfaction of the provided bound.
                           ","['Scukins, Edvards', 'Ogren, Petter']","['KTH', 'Royal Institute of Technology (KTH)']"
A Novel Hybrid Approach for Fault-Tolerant Control of UAVs Based on Robust Reinforcement Learning,"The control of complex autonomous systems has significantly improved in recent years and unmanned aerial vehicles (UAVs) have become popular in the research community. Although the use of UAVs is increasing, much work remains to guarantee fault-tolerant control (FTC) properties of these vehicles. Model-based controllers are the standard way to control UAVs, however, obtaining models of the system and environment for every possible operating condition a UAV can experience in a real-world scenario is not feasible.Reinforcement Learning has shown promise in controlling complex systems but requires training in a simulator (requiring a model) of the system. Further, stability guarantees do not exist for learning-based controllers, which limits their large-scale application in the real world. We propose a novel hybrid FTC approach that uses a learned supervisory controller (together with low-level PID controllers) with key stability guarantees. We use a robust reinforcement learning approach to learn the supervisory control parameters and prove stability. We empirically validate our framework using trajectory-following experiments (in simulation) for a quadcopter subject to rotor faults, wind disturbances, and severe position and attitude noise.
                           ","['Sohege, Yves', 'Quinones-Grueiro, Marcos', 'Provan, Gregory']","['Insight-Centre for Data Analytics', 'Vanderbilt University', 'University College Cork']"
Visual Navigation in Real-World Indoor Environments Using End-To-End Deep Reinforcement Learning,"Visual navigation is essential for many applications in robotics, from manipulation, through mobile robotics to automated driving. Deep reinforcement learning (DRL) provides an elegant map-free approach integrating image processing, localization, and planning in one module, which can be trained and therefore optimized for a given environment. However, to date, DRL-based visual navigation was validated exclusively in simulation, where the simulator provides information that is not available in the real world, e.g., the robot's position or segmentation masks. This precludes the use of the learned policy on a real robot. Therefore, we present a novel approach that enables a direct deployment of the trained policy on real robots. We have designed a new powerful simulator capable of domain randomization. To facilitate the training, we propose visual auxiliary tasks and a tailored reward scheme. The policy is fine-tuned on images collected from real-world environments. We have evaluated the method on a mobile robot in a real office environment. The training took approximately 30 hours on a single GPU. In 30 navigation experiments, the robot reached a 0.3-meter neighbourhood of the goal in more than 86.7% of cases. This result makes the proposed method directly applicable to tasks like mobile manipulation.
                           ","['Kulhanek, Jonas', 'Derner, Erik', 'Babuska, Robert']","['Charles University Prague', 'Czech Technical University in Prague', 'Delft University of Technology']"
Embodied Visual Navigation with Automatic Curriculum Learning in Real Environments,"We present NavACL, a method of automatic curriculum learning tailored to the navigation task. NavACL is simple to train and efficiently selects relevant tasks using geometric features. In our experiments, deep reinforcement learning agents trained using NavACL in collision-free environments significantly outperform state-of-the-art agents trained with uniform sampling -- the current standard. Furthermore, our agents are able to navigate through unknown cluttered indoor environments to semantically-specified targets using only RGB images. Collision avoidance policies and frozen feature networks support transfer to unseen real-world environments, without any modification or retraining requirements. We evaluate our policies in simulation, and in the real world on a ground robot and a quadrotor drone. Videos of real-world results are available in the supplementary material.
                           ","['Morad, Steven', 'Mecca, Roberto', 'Poudel, Rudra', 'Liwicki, Stephan', 'Cipolla, Roberto']","['The University of Cambridge', 'Computer Vision Group - Toshiba Europe', 'Toshiba Research', 'Toshiba Europe Limited', 'Cam']"
Robotic Imitation of Human Assembly Skills Using Hybrid Trajectory and Force Learning,"Robotic assembly tasks involve complex and low-clearance insertion trajectories with varying contact forces at different stages. While the nominal motion trajectory can be easily obtained from human demonstrations, the force profile is harder to obtain especially when a real robot is unavailable. It is difficult to obtain a realistic force profile in simulation even with physics engines. Such simulated force profiles tend to be unsuitable for the actual robotic assembly due to the reality gap and uncertainty in the assembly process. To address this problem, we present a combined learning-based framework to imitate human assembly skills through hybrid trajectory learning and force learning. The main contribution of this work is the development of a framework that combines hierarchical imitation learning, to learn the nominal motion trajectory, with a reinforcement learning-based force control scheme to learn an optimal force control policy. To further improve the imitation learning part, we develop a hierarchical architecture, following the idea of goal-conditioned imitation learning, to generate the trajectory learning policy on the skill level offline. Through experimental validations, we corroborate that the proposed learning-based framework is robust to uncertainty in the assembly task, can generate high-quality trajectories, and can find suitable force control policies, which adapt to the task's force requirements more efficiently.
                           ","['Wang, Yan', 'Beltran-Hernandez, Cristian Camilo', 'Wan, Weiwei', 'Harada, Kensuke']","['Osaka University', 'Osaka University', 'Osaka University', 'Osaka University']"
Living Object Grasping Using Two-Stage Graph Reinforcement Learning,"Living objects are hard to grasp because they can actively dodge and struggle by writhing or deforming while or even prior to being contacted and modeling or predicting their responses to grasping is extremely difficult.This paper presents an algorithm based on reinforcement learning (RL) to attack this challenging problem. Considering the complexity of living object grasping, we divide the whole task into pre-grasp and in-hand stages and let the algorithm switch stages automatically. The pre-grasp stage is aimed at finding a good pose of a robot hand approaching a living object for performing a grasp. Dense reward functions are proposed for facilitating the learning of right hand actions based on the poses of both hand and object. Since an object held in hand may struggle to escape, the robot hand needs to adjust its configuration and respond correctly to the object's movement.Hence, the goal of the in-hand stage is to determine an appropriate adjustment of finger configuration in order for the robot hand to keep holding the object. At this stage, we treat the robot hand as a graph and use the graph convolutional network (GCN) to determine the hand action. We test our algorithm with both simulation and real experiments, which show its good performance in living object grasping. More results are available on our website: https://sites.google.com/view/graph-rl.
                           ","['Hu, Zhe', 'Zheng, Yu', 'Pan, Jia']","['City University of Hong Kong', 'Tencent', 'University of Hong Kong']"
Reinforcement Learning for Robotic Assembly Using Non-Diagonal Stiffness Matrix,"Contact-rich tasks, wherein multiple contact transitions occur in a series of operations, are extensively studied for task automation. Precision assembly, one of the typical examples of contact-rich tasks, requires high time constants to cope with the change in contact state. This paper therefore proposes a local trajectory optimization method for precision assembly with high time constants. Since the non-diagonal components of the stiffness matrix is effective for inducing motion at high sampling frequency, we design a stiffness matrix to guide motion and propose a method to control a peg through the selection of the stiffness matrix. We introduced reinforcement learning (RL) for the selection of the stiffness matrix, because the relationship between the direction to be guided and the sensor response is difficult to model. The architecture with different sampling rates for RL and admittance control has an advantage of rapid response owing to the high time constant of the local trajectory optimization. The effectiveness of the method was verified via experiments involving two contact-rich tasks. The average total time of peg insertion was 1.64 s, which is less than the half the time reported by the best of the existing state of the art studies.
                           ","['Oikawa, Masahide', 'Kusakabe, Tsukasa', 'Kutsuzawa, Kyo', 'Sakaino, Sho', 'Tsuji, Toshiaki']","['Saitama University', 'Saitama University', 'Tohoku University', 'University of Tsukuba', 'Saitama University']"
Uncertainty-Aware Contact-Safe Model-Based Reinforcement Learning,"This paper presents contact-safe Model-based Reinforcement Learning (MBRL) for robot applications that achieves contact-safe behaviors in the learning process. In typical MBRL, we cannot expect the data-driven model to generate accurate and reliable policies to the intended robotic tasks during the learning process due to data scarcity. Operating these unreliable policies in a contact-rich environment could cause damage to the robot and its surroundings. To alleviate the risk of causing damage through unexpected intensive physical contacts, we present the contact-safe MBRL that associates the probabilistic Model Predictive Control's (pMPC) control limits with the model uncertainty so that the allowed acceleration of controlled behavior is adjusted according to learning progress. Control planning with such uncertainty-aware control limits is formulated as a deterministic MPC problem using a computationally-efficient approximated GP dynamics and an approximated inference technique. Our approach's effectiveness is evaluated through bowl mixing tasks with simulated and real robots, scooping tasks with a real robot as examples of contact-rich manipulation skills.
                           ","['Kuo, Cheng-Yu', 'Schaarschmidt, Andreas', 'Cui, Yunduan', 'Asfour, Tamim', 'Matsubara, Takamitsu']","['Nara Institute of Science and Technology', 'Karlsruhe University of Technology', 'Shenzhen Institutes of Advanced Technology, Chinese Academy of Sc', 'Karlsruhe Institute of Technology (KIT)', 'Nara Institute of Science and Technology']"
Reinforcement Learning-Based Visual Navigation with Information-Theoretic Regularization,"To enhance the cross-target and cross-scene generalization of target-driven visual navigation based on deep reinforcement learning(RL), we introduce an information-theoretic regularization term into the RL objective. The regularization maximizes the mutual information between navigation actions and visual observation transforms of an agent, thus promoting more informed navigation decisions. This way, the agent models the action-observation dynamics by learning a variational generative model. Based on the model, the agent generates (imagines) the next observation from its current observation and navigation target. This way, the agent learns to understand the causality between navigation actions and the changes in its observations, finally embodied in predicting the next action for navigation via comparing the current and the imagined next observations. Cross-target and cross-scene evaluations on the AI2-THOR framework show that our method attains at least a 10% improvement of average success rate over some state-of-the-art models. We further evaluate our model in two real-world settings: navigation in unseen indoor scenes from a discrete Active Vision Dataset (AVD) and continuous real-world environments with a TurtleBot. We demonstrate that our navigation model is able to successfully achieve navigation tasks in these scenarios. Videos and models can be found in the supplementary material.
                           ","['Wu, Qiaoyun', 'Xu, Kai', 'Wang, Jun', 'Xu, Mingliang', 'Gong, Xiaoxi', 'Manocha, Dinesh']","['Nanjing University of Aeronautics and Astronautics', 'National University of Defense Technology', 'Nanjing University of Aeronautics and Astronautics', 'National University of Defense Technology', 'Nanjing University of Aeronautics and Astronautics', 'University of Maryland']"
Deep Reinforcement Learning for Concentric Tube Robot Control with a Goal-Based Curriculum,"Concentric Tube Robots (CTRs), a type of continuum robot, are a collection of concentric, pre-curved tubes composed of super elastic nickel titanium alloy. CTRs can bend and twist from the interactions between neighboring tubes causing the kinematics and therefore control of the end-effector to be very challenging to model. In this paper, we develop a control scheme for a CTR end-effector in Cartesian space with no prior kinematic model using a deep reinforcement learning (DRL) approach with a goal-based curriculum reward strategy. We explore the use of curricula by changing the goal tolerance through training with constant, linear and exponential decay functions. Also, relative and absolute joint representations as a way of improving training convergence are explored. Quantitative comparisons for combinations of curricula and joint representations are performed and the exponential decay relative approach is used for training a robust policy in a noise-induced simulation environment. Compared to a previous DRL approach, our new method reduces training time and employs a more complex simulation environment. We report mean Cartesian errors of 1.29 mm and a success rate of 0.93 with a relative decay curriculum. In path following, we report mean errors of 1.37 mm in a noise-induced path following task. Albeit in simulation, these results indicate the promise of using DRL in model free control of continuum robots and CTRs in particular.
                           ","['Iyengar, Keshav Kannan', 'Stoyanov, Danail']","['University College London', 'University College London']"
DeepWalk: Omnidirectional Bipedal Gait by Deep Reinforcement Learning,"Bipedal walking is one of the most difficult but exciting challenges in robotics. The difficulties arise from the complexity of high-dimensional dynamics, sensing and actuation limitations combined with real-time and computational constraints. Deep Reinforcement Learning (DRL) holds the promise to address these issues by fully exploiting the robot dynamics with minimal craftsmanship. In this paper, we propose a novel DRL approach that enables an agent to learn omnidirectional locomotion for humanoid (bipedal) robots. Notably, the locomotion behaviors are accomplished by a single control policy (a single neural network). We achieve this by introducing a new curriculum learning method that gradually increases the task difficulty by scheduling target velocities. In addition, our method does not require reference motions which facilities its application to robots with different kinematics, and reduces the overall complexity. At the end, different strategies for sim-to-real transfer are presented which allow us to transfer the learned policy to real hardware.
                           ","['Rodriguez, Diego', 'Behnke, Sven']","['University of Bonn', 'University of Bonn']"
Efficient Self-Supervised Data Collection for Offline Robot Learning,"A practical approach to robot reinforcement learning is to first collect a large batch of real or simulated robot interaction data, using some data collection policy, and then learn from this data to perform various tasks, using offline learning algorithms. Previous work focused on manually designing the data collection policy, and on tasks where suitable policies can easily be designed, such as random picking policies for collecting data about object grasping. For more complex tasks, however, it may be difficult to find a data collection policy that explores the environment effectively, and produces data that is diverse enough for the downstream task. In this work, we propose that data collection policies should actively explore the environment to collect diverse data. In particular, we develop a simple-yet-effective goal-conditioned reinforcement-learning method that actively focuses data collection on novel observations, thereby collecting a diverse data-set. We evaluate our method on simulated robot manipulation tasks with visual inputs and show that the improved diversity of active data collection leads to significant improvements in the downstream learning tasks.
                           ","['Endrawis, Shadi', 'Leibovich, Gal', 'Jacob, Guy', 'Novik, Gal', 'Tamar, Aviv']","['Intel Labs, Technion - Israel Institute of Technology', 'Intel Labs', 'Intel', 'Intel Labs', 'UC Berkeley']"
Learning Stable Normalizing-Flow Control for Robotic Manipulation,"Reinforcement Learning (RL) of robotic manipulation skills, despite its impressive successes, stands to benefit from incorporating domain knowledge from control theory. One of the most important properties that is of interest is control stability. Ideally, one would like to achieve stability guarantees while staying within the framework of state-of-the-art deep RL algorithms. Such a solution does not exist in general, especially one that scales to complex manipulation tasks. We contribute towards closing this gap by introducing normalizing-flow control structure, that can be deployed in any latest deep RL algorithms. While stable exploration is not guaranteed, our method is designed to ultimately produce deterministic controllers with provable stability. In addition to demonstrating our method on challenging contact-rich manipulation tasks, we also show that it is possible to achieve considerable exploration efficiency¨Creduced state space coverage and actuation efforts¨Cwithout losing learning efficiency.
                           ","['Abdul Khader, Shahbaz', 'Yin, Hang', 'Falco, Pietro', 'Kragic, Danica']","['ABB Corporate Research', 'KTH', 'ABB, Corporate Research', 'KTH']"
How to Train Your Heron,"In this paper we apply Deep Reinforcement Learning (Deep RL) and Domain Randomization to solve a navigation task in a natural environment relying solely on a 2D laser scanner. We train a model-based RL agent in simulation to follow lake and river shores and apply it on a real Unmanned Surface Vehicle in a zero-shot setup. We demonstrate that even though the agent has not been trained in the real world, it can fulfill its task successfully and adapt to changes in the robot's environment and dynamics. Finally, we show that the RL agent is more robust, faster, and more accurate than a state-aware Model-Predictive-Controller. Code, simulation environments, pre-trained models, and datasets are available at https://github.com/AntoineRichard/Heron-RL-ICRA.git.
                           ","['Richard, Antoine', 'Aravecchia, Stephanie', 'Schillaci, Thomas', 'Geist, Matthieu', 'Pradalier, Cedric']","['Georgia Institute of Technology', 'Georgia Tech Lorraine - UMI 2958 GT-CNRS', 'Georgia Institute of Technology', 'Universit¨¦ De Lorraine', 'GeorgiaTech Lorraine']"
A Metric Space Perspective on Self-Supervised Policy Adaptation,"One of the most challenging aspects of real-world reinforcement learning (RL) is the multitude of unpredictable and ever-changing distractions that could divert an agent from what was tasked to do in its training environment. While an agent could learn from reward signals to ignore them, the complexity of the real-world can make rewards hard to acquire, or, at best, extremely sparse. A recent class of self-supervised methods have shown promise that reward-free adaptation under challenging distractions is possible. However, previous work focused on a short one-episode adaptation setting. In this paper, we consider a long-term adaptation setup that is more akin to the specifics of the real-world and propose a metric space perspective on self-supervised adaptation. We empirically describe the processes that take place in the embedding space during this adaptation process, reveal some of its undesirable effects on performance and show how they can be eliminated. Moreover, we theoretically study how actor-based and actor-free agents can further generalise to the target environment by manipulating the Lipschitz constant of the actor and critic functions.
                           ","['Bodnar, Cristian', 'Hausman, Karol', 'Dulac-Arnold, Gabriel', 'Jonschkowski, Rico']","['University of Cambridge', 'Google Brain', 'Google', 'Google']"
Self-Imitation Learning by Planning,"Imitation learning (IL) enables robots to acquire skills quickly by transferring expert knowledge, which is widely adopted in reinforcement learning (RL) to initialize exploration. However, in long-horizon motion planning tasks, a challenging problem in deploying IL and RL methods is how to generate and collect massive, broadly distributed data such that these methods can generalize effectively. In this work, we solve this problem using our proposed approach called self-imitation learning by planning (SILP), where demonstration data are collected automatically by planning on the visited states from the current policy. SILP is inspired by the observation that successfully visited states in the early reinforcement learning stage are collision-free nodes in the graph-search based motion planner, so we can plan and relabel robot's own trials as demonstrations for policy learning. Due to these self-generated demonstrations, we relieve human from the laborious data preparation process required by IL and RL methods in solving complex motion planning tasks. The evaluation results show that our SILP method achieves higher success rates and enhances sample efficiency compared to selected baselines, and the policy learned in simulation performs well in a real-world placement task with changing goals and obstacles.
                           ","['Luo, Sha', 'Kasaei, Hamidreza', 'Schomaker, Lambert R.B.']","['University of Groningen', 'University of Groningen', 'University of Groningen']"
Learning Robot Trajectories Subject to Kinematic Joint Constraints,"We present an approach to learn fast and dynamic robot motions without exceeding limits on the position, velocity, acceleration and jerk of each robot joint. Movements are generated by mapping the predictions of a neural network to safely executable joint accelerations. The neural network is invoked periodically and trained via reinforcement learning. Our main contribution is an analytical procedure for calculating safe joint accelerations, which considers the prediction frequency f_N of the neural network. As a result, the frequency f_N can be freely chosen and treated as a hyperparameter. We show that our approach is preferable to penalizing constraint violations as it provides explicit guarantees and does not distort the desired optimization target. In addition, the influence of the selected prediction frequency on the learning performance and on the computing effort is highlighted by various experiments.
                           ","['Kiemel, Jonas', 'Kroeger, Torsten']","['Karlsruhe Institute of Technology', 'Karlsruher Institut F¨¹r Technologie (KIT)']"
Representation Matters: Improving Perception and Exploration for Robotics,"Projecting high-dimensional environment observations into lower-dimensional structured representations can considerably improve data-efficiency for reinforcement learning in domains with limited data such as robotics. Can a single generally useful representation be found? In order to answer this question, it is important to understand how the representation will be used by the agent and what properties such a ¡®good¡¯ representation should have. In this paper we systematically evaluate a number of common learnt and hand-engineered representations in the context of three robotics tasks: lifting, stacking and pushing of 3D blocks. The representations are evaluated in two use-cases: as input to the agent, or as a source of auxiliary tasks. Furthermore, the value of each representation is evaluated in terms of three properties: dimensionality, observability and disentanglement. We can significantly improve performance in both use-cases and demonstrate that some representations can perform commensurate to simulator states as agent inputs. Finally, our results challenge common intuitions by demonstrating that: 1) dimensionality strongly matters for task generation, but is negligible for inputs, 2) observability of task-relevant aspects mostly affects the input representation use-case, and 3) disentanglement leads to better auxiliary tasks, but has only limited benefits for input representations.
                           ","['Wulfmeier, Markus', 'Byravan, Arunkumar', 'Hertweck, Tim', 'Higgins, Irina', 'Gupta, Ankush', 'Kulkarni, Tejas', 'Reynolds, Malcolm', 'Teplyashin, Denis', 'Hafner, Roland', 'Lampe, Thomas', 'Riedmiller, Martin']","['University of Oxford', 'University of Washington', 'DeepMind', 'DeepMind', 'Google UK', 'DeepMind', 'DeepMind', 'DeepMind', 'Google DeepMind', 'Google UK Ltd', 'DeepMind']"
NavRep: Unsupervised Representations for Reinforcement Learning of Robot Navigation in Dynamic Human Environments,"Robot navigation is a task where reinforcement learning approaches are still unable to compete with traditional path planning. State-of-the-art methods differ in small ways, and do not all provide reproducible, openly available implementations. This makes comparing methods a challenge. Recent research has shown that unsupervised learning methods can scale impressively, and be leveraged to solve difficult problems. In this work, we design ways in which unsupervised learning can be used to assist reinforcement learning for robot navigation. We train two end-to-end, and 18 unsupervised-learning-based architectures, and compare them, along with existing approaches, in unseen test cases. We demonstrate our approach working on a real life robot. Our results show that unsupervised learning methods are competitive with end-to-end methods. We also highlight the importance of various components such as input representation, predictive unsupervised learning, and latent features. We make all our models publicly available, as well as training and testing environments, and tools. This release also includes OpenAI-gym-compatible environments designed to emulate the training conditions described by other papers, with as much fidelity as possible. Our hope is that this helps in bringing together the field of RL for robot navigation, and allows meaningful comparisons across state-of-the-art methods.
                           ","['Dugas, Daniel', 'Nieto, Juan', 'Siegwart, Roland', 'Chung, Jen Jen']","['ETH Zurich', 'ETH Z¨¹rich', 'ETH Zurich', 'Eidgen?ssische Technische Hochschule Z¨¹rich']"
A General Framework to Increase Safety of Learning Algorithms for Dynamical Systems Based on Region of Attraction Estimation (I),"Although the state-of-the-art learning approaches exhibit impressive results for dynamical systems, only a few applications on real physical systems have been presented. One major impediment is that the intermediate policy during the training procedure may result in behaviors that are not only harmful to the system itself but also to the environment. In essence, imposing safety guarantees for learning algorithms is vital for autonomous systems acting in the real world. In this article, we propose a computationally effective and general safe learning framework, specifically for complex dynamical systems. With a proper definition of the safe region, a supervisory control strategy, which switches the actions applied on the system between the learning-based controller and a predefined corrective controller, is given. A simplified system facilitates the estimation of the safe region for the high-dimensional dynamical system. During the learning phase, the belief of the safe region is updated with the actual execution results of the corrective controller, which in turn enables the learning-based controller to have more freedom in choosing its actions. Two examples are given to demonstrate the performance of the proposed framework, one simple inverted pendulum to illustrate the online adaptation method, and one quadcopter control task to show the overall performance.
                           ","['Zhou, Zhehua', 'Oguz, Ozgur S.', 'Leibold, Marion', 'Buss, Martin']","['Technical University of Munich', 'Uni of Stuttgart & Max Planck Inst. for Intelligent Systems', 'Technische Universit?t M¨¹nchen', 'Technische Universit?t M¨¹nchen']"
Decentralized Multi-Agent Pursuit Using Deep Reinforcement Learning,"Pursuit-evasion is the problem of capturing mobile targets with one or more pursuers. We use deep reinforcement learning for pursuing an omnidirectional target with multiple, homogeneous agents that are subject to unicycle kinematic constraints. We use shared experience to train a policy for a given number of pursuers, executed independently by each agent at run-time. The training uses curriculum learning, a sweeping-angle ordering to locally represent neighboring agents, and a reward structure that encourages a good formation and combines individual and group rewards. Simulated experiments with a reactive evader and up to eight pursuers show that our learning-based approach outperforms recent reinforcement learning techniques as well as non-holonomic adaptations of classical algorithms. The learned policy is successfully transferred to the real-world in a proof-of-concept demonstration with three motion-constrained pursuer drones.
                           ","['De Souza Jr., Cristino', 'Newbury, Rhys', 'Cosgun, Akansel', 'Castillo, Pedro', 'Vidolov, Borislav', 'Kulic, Dana']","['Universite De Technologie De Compiegne', 'Monash University', 'Monash University', 'Sorbonne Universit¨¦s, Universit¨¦ De Technologie De Compi¨¨gne', 'Universit¨¦ De Technologie De Compi¨¨gne', 'Monash University']"
Distilling a Hierarchical Policy for Planning and Control Via Representation and Reinforcement Learning,"We present a hierarchical planning and control framework that enables an agent to perform various tasks and adapt to a new task flexibly. Rather than learning an individual policy for each particular task, the proposed framework, DISH, distills a hierarchical policy from a set of tasks by representation and reinforcement learning. The framework is based on the idea of latent variable models that represent high-dimensional observations using low-dimensional latent variables. The resulting policy consists of two levels of hierarchy: (i) a planning module that reasons a sequence of latent intentions that would lead to an optimistic future and (ii) a feedback control policy, shared across the tasks, that executes the inferred intention. Because the planning is performed in low-dimensional latent space, the learned policy can immediately be used to solve or adapt to new tasks without additional training. We demonstrate the proposed framework can learn compact representations (3- and 1-dimensional latent states and commands for a humanoid with 197- and 36-dimensional state features and actions) while solving a small number of imitation tasks, and the resulting policy is directly applicable to other types of tasks, i.e., navigation in cluttered environments.
                           ","['Ha, Jung-Su', 'Park, Young-Jin', 'Chae, Hyeok-Joo', 'Park, Soon-Seo', 'Choi, Han-Lim']","['Technical University of Berlin', 'Naver R&D Center, NAVER Corp', 'KAIST', 'Korea Advanced Institute of Science and Technology', 'KAIST']"
Distributed Heuristic Multi-Agent Path Finding with Communication,"Multi-Agent Path Finding (MAPF) is essential to large-scale robotic systems. Recent methods have applied reinforcement learning (RL) to learn decentralized polices in partially observable environments. A fundamental challenge of obtaining collision-free policy is that agents need to learn cooperation to handle congested situations. This paper combines communication with deep Q-learning to provide a novel learning based method for MAPF, where agents achieve cooperation via graph convolution. To guide RL algorithm on long-horizon goal-oriented tasks, we embed the potential choices of shortest paths from single source as heuristic guidance instead of using a specific path as in most existing works. Our method treats each agent independently and trains the model from a single agent's perspective. The final trained policy is applied to each agent for decentralized execution. The whole system is distributed during training and is trained under a curriculum learning strategy. Empirical evaluation in obstacle-rich environment indicates the high success rate with low average step of our method.
                           ","['Ma, Ziyuan', 'Luo, Yudong', 'Ma, Hang']","['Simon Fraser University', 'University of Waterloo', 'Simon Fraser University']"
Reducing the Deployment-Time Inference Control Costs of Deep Reinforcement Learning Agents Via an Asymmetric Architecture,"Deep reinforcement learning (DRL) has been demonstrated to provide promising results in several challenging decision making and control tasks. However, the required inference costs of deep neural networks (DNNs) could prevent DRL from being applied to mobile robots which cannot afford high energy-consuming computations. To enable DRL methods to be affordable in such energy-limited platforms, we propose an asymmetric architecture that reduces the overall inference costs via switching between a computationally expensive policy and an economic one. The experimental results evaluated on a number of representative benchmark suites for robotic control tasks demonstrate that our method is able to reduce the inference costs while retaining the agent's overall performance.
                           ","['Chang, Chin-Jui', 'Chu, Yu-Wei', 'Ting, Chao-Hsien', 'Liu, Hao Kang', 'Hong, Zhang-Wei', 'Lee, Chun-Yi']","['Academia Sinica', 'National Tsing Hua University', 'National Tsing Hua University', 'National Tsing Hau University', 'National Tsing Hua University', 'National Tsing Hua University']"
Adversarial Skill Learning for Robust Manipulation,"Deep reinforcement learning has made significant progress in robotic manipulation tasks and it works well in the ideal disturbance-free environment. However, in a real-world environment, both internal and external disturbances are inevitable, thus the performance of the trained policy will dramatically drop. To improve the robustness of the policy, we introduce the adversarial training mechanism to the robotic manipulation tasks in this paper, and an adversarial skill learning algorithm based on soft actor-critic (SAC) is proposed for robust manipulation. Extensive experiments are conducted to demonstrate that the learned policy is robust to internal and external disturbances. Additionally, the proposed algorithm is evaluated in both the simulation environment and on the real robotic platform.
                           ","['Jian, Pingcheng', 'Yang, Chao', 'Guo, Di', 'Liu, Huaping', 'Sun, Fuchun']","['Tsinghua', 'Tsinghua University', 'Tsinghua University', 'Tsinghua University', 'Tsinghua Univerisity']"
Learning Sequences of Manipulation Primitives for Robotic Assembly,"This paper explores the idea that skillful assembly is best represented as dynamic sequences of Manipulation Primitives, and that such sequences can be automatically discovered by Reinforcement Learning. Manipulation Primitives, such as ``Move down until contact'', ``Slide along x while maintaining contact with the surface'', have enough complexity to keep the search tree shallow, yet are generic enough to generalize across a wide range of assembly tasks. Moreover, the additional ``semantics'' of the Manipulation Primitives make them more robust in sim2real and against model/environment variations and uncertainties, as compared to more elementary actions. Policies are learned in simulation, and then transferred onto the physical platform. Direct sim2real transfer (without retraining in real) achieves excellent success rates on challenging assembly tasks, such as round peg insertion with 0.04 mm clearance or square peg insertion with large hole position/orientation estimation errors.
                           ","['Vuong, Nghia', 'Pham, Hung', 'Pham, Quang-Cuong']","['Nanyang Technological University', 'Nanyang Technological University', 'NTU Singapore']"
Multi-Modal Mutual Information (MuMMI) Training for Robust Self-Supervised Deep Reinforcement Learning,"This work focuses on learning useful and robust deep world models using multiple, possibly unreliable, sensors. We find that current methods do not sufficiently encourage a shared representation between modalities; this can cause poor performance on downstream tasks and over-reliance on specific sensors. As a solution, we contribute a new multi-modal deep latent state-space model, trained using a mutual information lower-bound. The key innovation is a specially-designed density ratio estimator that encourages consistency between the latent codes of each modality. We tasked our method to learn policies (in a self-supervised manner) on multi-modal Natural MuJoCo benchmarks and a challenging Table Wiping task. Experiments show our method significantly outperforms state-of-the-art deep reinforcement learning methods, particularly in the presence of missing observations.
                           ","['Chen, Kaiqi', 'Lee, Yong', 'Soh, Harold']","['National University of Singapore', 'National University of Singapore', 'National Universtiy of Singapore']"
TRANS-AM: Transfer Learning by Aggregating Dynamics Models for Soft Robotic Assembly,"Practical industrial assembly scenarios often require robotic agents to adapt their skills to unseen tasks quickly. While transfer reinforcement learning (RL) could enable such quick adaptation, much prior work has to collect many samples from source environments to learn target tasks in a model-free fashion, which still lacks sample efficiency on a practical level. In this work, we develop a novel transfer RL method named TRANSfer learning by Aggregating dynamics Models (TRANS-AM). TRANS-AM is based on model-based RL (MBRL) for its high-level sample efficiency, and only requires dynamics models to be collected from source environments. Specifically, it learns to aggregate source dynamics models adaptively in an MBRL loop to better fit the state-transition dynamics of target environments and execute optimal actions there. As a case study to show the effectiveness of this proposed approach, we address a challenging contact-rich peg-in-hole task with variable hole orientations using a soft robot. Our evaluations with both simulation and real-robot experiments demonstrate that TRANS-AM enables the soft robot to accomplish target tasks with fewer episodes compared when learning the tasks from scratch.
                           ","['Tanaka, Kazutoshi', 'Yonetani, Ryo', 'Hamaya, Masashi', 'Lee, Robert', 'von Drigalski, Felix Wolf Hans Erich', 'Ijiri, Yoshihisa']","['OMRON SINIC X Corporation', 'Omron Sinic X', 'OMRON SINIC X Corporation', 'Australian Centre for Robotic Vision', 'OMRON SINIC X Corporation', 'OMRON Corp']"
Proximal Policy Optimization with Relative Pearson Divergence,"The recent remarkable progress of deep reinforcement learning (DRL) stands on regularization of policy for stable and efficient learning. A popular method, named proximal policy optimization (PPO), has been introduced for this purpose. PPO clips density ratio of the latest and baseline policies with a threshold, while its minimization target is unclear. As another problem of PPO, the symmetric threshold is given numerically while the density ratio itself is in asymmetric domain, thereby causing unbalanced regularization of the policy. This paper therefore proposes a new variant of PPO by considering a regularization problem of relative Pearson (RPE) divergence, so-called PPO-RPE. This regularization yields the clear minimization target, which constrains the latest policy to the baseline one. Through its analysis, the intuitive threshold-based design consistent with the asymmetry of the threshold and the domain of density ratio can be derived. Through four benchmark tasks, PPO-RPE performed as well as or better than the conventional methods in terms of the task performance by the learned policy.
                           ","['Kobayashi, Taisuke']",['Nara Institute of Science and Technology']
A Coach-Based Bayesian Reinforcement Learning Method for Snake Robot Control,"Reinforcement Learning (RL) usually needs thousands of episodes, leading its applications on physical robots expensive and challenging. Little research has been reported about snake robot control using RL due to additional difficulty of high redundancy of freedom. We propose a coach-based deep learning method for snake robot control, which can effectively save convergence time with much less episodes. The main contributions include: 1) a unified graph-based Bayesian framework integrating a coach module to guide the RL agent; 2) an explicit stochastic formulation of robot-environment interaction with uncertainty; 3) an efficient and robust training process for snake robot control to achieve both path planning and obstacle avoidance simultaneously. The performance has been demonstrated on both simulation and real-world data in comparison with state-of-the art, showing promising results.
                           ","['Jia, Yuanyuan', 'Ma, Shugen']","['Ritsumeikan University', 'Ritsumeikan University']"
Hyperparameter Auto-Tuning in Self-Supervised Robotic Learning,"Policy optimization in reinforcement learning requires the selection of numerous hyperparameters across different environments. Fixing them incorrectly may negatively impact optimization performance leading notably to insufficient or redundant learning. Insufficient learning (due to convergence to local optima) results in under-performing policies whilst redundant learning wastes time and resources. The effects are further exacerbated when using single policies to solve multi-task learning problems. In this paper, we study how the Evidence Lower Bound (ELBO) used in Variational Auto-Encoders (VAEs) is affected by the diversity of image samples. Different tasks or setups in visual reinforcement learning incur varying diversity. We exploit the ELBO to create an auto-tuning technique in self-supervised reinforcement learning. Our approach can auto-tune three hyperparameters: the replay buffer size, the number of policy gradient updates during each epoch, and the number of exploration steps during each epoch. We use the state-of-the-art self-supervised robotic learning framework (Reinforcement Learning with Imagined Goals (RIG) using Soft Actor-Critic) as baseline for experimental verification. Experiments show that our method can auto-tune online and yields the best performance at a fraction of the time and computational resources. Code, video, and appendix for simulated and real-robot experiments can be found at url{www.JuanRojas.net/autotune}.
                           ","['Huang, Jiancong', 'Rojas, Juan', 'Zimmer, Matthieu', 'Wu, Hongmin', 'Guan, Yisheng', 'Weng, Paul']","['Guangdong University of Technology', 'Chinese University of Hong Kong', 'Shanghai Jiao Tong University', 'Guangdong Institute of Intelligent Manufacturing', 'Guangdong University of Technology', 'Shanghai Jiao Tong University']"
Sim-To-Real Visual Grasping Via State Representation Learning Based on Combining Pixel-Level and Feature-Level Domain Adaptation,"In this study, we present a method to grasp diverse unseen real-world objects using an off-policy actor-critic deep reinforcement learning (RL) with the help of a simulation and the use of as little real-world data as possible. Actor-critic deep RL is unstable and difficult to tune when a raw image is given as an input. Therefore, we use state representation learning (SRL) to make actor-critic RL feasible for visual grasping tasks. Meanwhile, to reduce visual reality gap between simulation and reality, we also employ a typical pixel-level domain adaptation that can map simulated images to realistic ones. In our method, as the SRL model is a common preprocessing module for simulated and real-world data, we perform SRL using real and adapted images. This pixel-level domain adaptation enables the robot to learn grasping skills in a real environment using small amounts of real-world data. However, the controller trained in the simulation should adapt to the real world efficiently. Hence, we propose a method combining a typical pixel-level domain adaptation and the proposed SRL model, where we perform SRL based on a feature-level domain adaptation. In evaluations of vision-based robotics grasping tasks, we show that the proposed method achieves a substantial improvement over a method that only employs a pixel-level or domain adaptation.
                           ","['Suh, Il Hong', 'Park, Young-Bin', 'Lee, Sang Hyoung']","['Hanyang University', 'Hanyang University', 'Korea Institute of Industrial Technology']"
Observation Space Matters: Benchmark and Optimization Algorithm,"Recent advances in deep reinforcement learning (deep RL) enable researchers to solve challenging control problems, from simulated environments to real-world robotic tasks. However, deep RL algorithms are known to be sensitive to the problem formulation, including observation spaces, action spaces, and reward functions. There exist numerous choices for observation spaces but they are often designed solely based on prior knowledge due to the lack of established principles. In this work, we conduct benchmark experiments to verify common design choices for observation spaces, such as Cartesian transformation, binary contact flags, a short history, or global positions. Then we propose a search algorithm to find the optimal observation spaces, which examines various candidate observation spaces and removes unnecessary observation channels with a Dropout-Permutation test. We demonstrate that our algorithm significantly improves learning speed compared to manually designed observation spaces. We also analyze the proposed algorithm by evaluating different hyperparameters.
                           ","['Kim, Joanne Taery', 'Ha, Sehoon']","['Lawrence Livermore National Laboratory', 'Georgia Institute of Technology']"
A Peg-In-Hole Task Strategy for Holes in Concrete,"A method that enables an industrial robot to accomplish the peg-in-hole task for holes in concrete is proposed. The proposed method involves slightly detaching the peg from the wall, when moving between search positions, to avoid the negative influence of the concrete's high friction coefficient. It uses a deep neural network (DNN), trained via reinforcement learning, to effectively find holes with variable shape and surface finish (due to the brittle nature of concrete) without analytical modeling or control parameter tuning. The method uses displacement of the peg toward the wall surface, in addition to force and torque, as one of the inputs of the DNN. Since the displacement increases as the peg gets closer to the hole (due to the chamfered shape of holes in concrete), it is a useful parameter for inputting in the DNN. The proposed method was evaluated by training the DNN on a hole 500 times and attempting to find 12 unknown holes. The results of the evaluation show the DNN enabled a robot to find the unknown holes with average success rate of 96.1% and average execution time of 12.5 seconds. Additional evaluations with random initial positions and a different type of peg demonstrate the trained DNN can generalize well to different conditions. Analyses of the influence of the peg displacement input showed the success rate of the DNN is increased by utilizing this parameter. These results validate the proposed method in terms of its effectiveness in the construction field.
                           ","['Yasutomi, Andr¨¦ Yuji', 'Mori, Hiroki', 'Ogata, Tetsuya']","['Hitachi Ltd', 'Waseda University', 'Waseda University']"
Learning from Demonstration without Demonstrations,"State-of-the-art reinforcement learning (RL) algorithms suffer from high sample complexity, particularly in the sparse reward case. A popular strategy for mitigating this problem is to learn control policies by imitating a set of expert demonstrations. The drawback of such approaches is that an expert needs to produce demonstrations, which may be costly in practice. To address this shortcoming, we propose Probabilistic Planning for Demonstration Discovery (P2D2), a technique for automatically discovering demonstrations without access to an expert. We formulate discovering demonstrations as a search problem and leverage widely-used planning algorithms such as Rapidly-exploring Random Tree to find demonstration trajectories. These demonstrations are used to initialize a policy, then refined by a generic RL algorithm. We provide theoretical guarantees of P2D2 finding successful trajectories, as well as bounds for its sampling complexity. We experimentally demonstrate the method outperforms classic and intrinsic exploration RL techniques in a range of classic control and robotics tasks, requiring only a fraction of exploration samples and achieving better asymptotic performance.
                           ","['Blau, Tom', 'Morere, Philippe', 'Francis, Gilad']","['University of Sydney', 'University of Sydney', 'The University of Sydney']"
Dreaming: Model-Based Reinforcement Learning by Latent Imagination without Reconstruction,"In the present paper, we propose a decoder-free extension of Dreamer, a leading model-based reinforcement learning (MBRL) method from pixels. Dreamer is a sample- and cost-efficient solution to robot learning, as it is used to train latent state-space models based on a variational autoencoder and to conduct policy optimization by latent trajectory imagination. However, this autoencoding based approach often causes object vanishing, in which the autoencoder fails to perceives key objects for solving control tasks, and thus significantly limiting Dreamer's potential. This work aims to relieve this Dreamer's bottleneck and enhance its performance by means of removing the decoder. For this purpose, we firstly derive a likelihood-free and InfoMax objective of contrastive learning from the evidence lower bound of Dreamer. Secondly, we incorporate two components, (i) independent linear dynamics and (ii) the random crop data augmentation, to the learning scheme so as to improve the training performance. In comparison to Dreamer and other recent model-free reinforcement learning methods, our newly devised Dreamer with InfoMax and without generative decoder (Dreaming) achieves the best scores on 5 difficult simulated robotics tasks, in which Dreamer suffers from object vanishing.
                           ","['Okada, Masashi', 'Taniguchi, Tadahiro']","['Panasonic Corporation', 'Ritsumeikan University']"
Sample Efficient Reinforcement Learning Via Model-Ensemble Exploration and Exploitation,"Model-based deep reinforcement learning has achieved success in various domains that require high sample efficiencies, such as Go and robotics. However, there are some remaining issues, such as planning efficient explorations to learn more accurate dynamic models, evaluating the uncertainty of the learned models, and more rational utilization of models. To mitigate these issues, we present MEEE, a model-ensemble method that consists of optimistic exploration and weighted exploitation. During exploration, unlike prior methods directly selecting the optimal action that maximizes the expected accumulative return, our agent first generates a set of action candidates and then seeks out the optimal action that takes both expected return and future observation novelty into account. During exploitation, different discounted weights are assigned to imagined transition tuples according to their model uncertainty respectively, which will prevent model predictive error propagation in agent training. Experiments on several challenging continuous control benchmark tasks demonstrated that our approach outperforms other model-free and model-based state-of-the-art methods, especially in sample complexity.
                           ","['Yao, Yao', 'Xiao, Li', 'An, Zhicheng', 'Zhang, Wanpeng', 'Luo, Dijun']","['Tsinghua-Berkeley Shenzhen Institute, Tsinghua Shenzhen Internat', 'Tsinghua-Berkeley Shenzhen Institute, Tsinghua Shenzhen Internat', 'Tsinghua-Berkeley Shenzhen Institute, Tsinghua Shenzhen Internat', 'Tsinghua University', 'Tencent']"
Sample-Efficient Reinforcement Learning in Robotic Table Tennis,"Reinforcement learning (RL) has recently shown impressive success in various computer games and simulations. Most of these successes are based on numerous episodes to be learned from. For typical robotic applications, however, the number of feasible attempts is very limited. In this paper we present a sample-efficient RL algorithm applied to the example of a table tennis robot. In table tennis every stroke is different, of varying placement, speed and spin. Therefore, an accurate return has be found depending on a high-dimensional continuous state space. To make learning in few trials possible the method is embedded into our robot system. In this way we can use a one-step environment. The state space depends on the ball at hitting time (position, velocity, spin) and the action is the racket state (orientation, velocity) at hitting. An actor-critic based deterministic policy gradient algorithm was developed for accelerated learning. Our approach shows competitive performance in both simulation and on the real robot in different challenging scenarios. Accurate results are always obtained within under 200 episodes of training. A demonstration video is provided as supplementary material.
                           ","['Tebbe, Jonas', 'Krauch, Lukas', 'Gao, Yapeng', 'Zell, Andreas']","['University of T¨¹bingen', 'University of T¨¹bingen', 'University of Tuebingen', 'University of T¨¹bingen']"
